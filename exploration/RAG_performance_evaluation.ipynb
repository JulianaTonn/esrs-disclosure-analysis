{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa21bf06",
   "metadata": {},
   "source": [
    "# Evaluate the Performance of different RAG system settings\n",
    "\n",
    "Purpose of the notebook: evaluate the performance of different RAG settings to find the configuration that works best for this usecase\n",
    "\n",
    "Evaluate:\n",
    "- Embedding models\n",
    "- Retrieval settings\n",
    "- Retrieval queries\n",
    "- Generative LLM models\n",
    "- PDF Extraction\n",
    "- Generation prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de83772b",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5adf8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import re\n",
    "import urllib3\n",
    "import tenacity\n",
    "import configparser\n",
    "import markdown\n",
    "import json\n",
    "\n",
    "# for PDF extraction\n",
    "import pymupdf\n",
    "import requests\n",
    "import os\n",
    "import io\n",
    "\n",
    "\n",
    "# for retrieval\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "\n",
    "# for generation\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import tiktoken\n",
    "\n",
    "# old evaluation\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from thefuzz import process, fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f37cba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import OpenAI API key from environment variable\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cab153",
   "metadata": {},
   "source": [
    "# Sample reports\n",
    "Source: Sustainability Reporting Navigator (crowd-source list of CSRD-compliant reports for fiscal years starting on 01/01/2024)\n",
    "\n",
    "Downloaded CSV with information on all reports on the 08/04/2025 https://www.sustainabilityreportingnavigator.com/#/csrdreports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a637e83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277\n"
     ]
    }
   ],
   "source": [
    "# Open the csv data file\n",
    "reports_24 = pd.read_csv('esg_reports_2024.csv')\n",
    "print(len(reports_24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ddf5bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Unnamed: 0",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "company_withAccessInfo",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "link",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "country",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sector",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "industry",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "publication date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "pages PDF",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "auditor",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "8f2ca638-2928-45f3-94b9-a1895e84b2c4",
       "rows": [
        [
         "220",
         "19",
         "Continental AG",
         "https://annualreport.continental.com/2024/en/service/docs/annual-report-2024-data.pdf",
         "Germany",
         "Transportation",
         "Auto Parts",
         "2025-03-18",
         "125",
         "PwC"
        ],
        [
         "58",
         "266",
         "Schneider Electric*",
         "https://www.se.com/ww/en/assets/564/document/510443/2024-universal-registration-document.pdf?p_enDocType=Financial%20release&p_File_Name=Universal%20Registration%20Document%202024",
         "France",
         "Resource Transformation",
         "Electrical & Electronic Equipment",
         "2025-03-26",
         "186",
         "PwC & Mazars"
        ],
        [
         "89",
         "103",
         "Philips",
         "https://www.results.philips.com/publications/ar24/downloads/files/en/PhilipsFullAnnualReport2024-English.pdf?v%3D20250221111026&sa=D&source=editors&ust=1740414875713543&usg=AOvVaw03JVrsRo5pGqKO9bnEsL-L",
         "Netherlands",
         "Infrastructure",
         "Electric Utilities & Power Generators",
         "2025-02-21",
         "85",
         "EY"
        ]
       ],
       "shape": {
        "columns": 9,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>company_withAccessInfo</th>\n",
       "      <th>link</th>\n",
       "      <th>country</th>\n",
       "      <th>sector</th>\n",
       "      <th>industry</th>\n",
       "      <th>publication date</th>\n",
       "      <th>pages PDF</th>\n",
       "      <th>auditor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>19</td>\n",
       "      <td>Continental AG</td>\n",
       "      <td>https://annualreport.continental.com/2024/en/s...</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Transportation</td>\n",
       "      <td>Auto Parts</td>\n",
       "      <td>2025-03-18</td>\n",
       "      <td>125</td>\n",
       "      <td>PwC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>266</td>\n",
       "      <td>Schneider Electric*</td>\n",
       "      <td>https://www.se.com/ww/en/assets/564/document/5...</td>\n",
       "      <td>France</td>\n",
       "      <td>Resource Transformation</td>\n",
       "      <td>Electrical &amp; Electronic Equipment</td>\n",
       "      <td>2025-03-26</td>\n",
       "      <td>186</td>\n",
       "      <td>PwC &amp; Mazars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>103</td>\n",
       "      <td>Philips</td>\n",
       "      <td>https://www.results.philips.com/publications/a...</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>Infrastructure</td>\n",
       "      <td>Electric Utilities &amp; Power Generators</td>\n",
       "      <td>2025-02-21</td>\n",
       "      <td>85</td>\n",
       "      <td>EY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0 company_withAccessInfo  \\\n",
       "220          19         Continental AG   \n",
       "58          266    Schneider Electric*   \n",
       "89          103                Philips   \n",
       "\n",
       "                                                  link      country  \\\n",
       "220  https://annualreport.continental.com/2024/en/s...      Germany   \n",
       "58   https://www.se.com/ww/en/assets/564/document/5...       France   \n",
       "89   https://www.results.philips.com/publications/a...  Netherlands   \n",
       "\n",
       "                      sector                               industry  \\\n",
       "220           Transportation                             Auto Parts   \n",
       "58   Resource Transformation      Electrical & Electronic Equipment   \n",
       "89            Infrastructure  Electric Utilities & Power Generators   \n",
       "\n",
       "    publication date  pages PDF       auditor  \n",
       "220       2025-03-18        125           PwC  \n",
       "58        2025-03-26        186  PwC & Mazars  \n",
       "89        2025-02-21         85            EY  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randomly select 3 reports from 2024\n",
    "sample = reports_24.sample(n=3, random_state=3)\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1b7d3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_list = ['ContinentalAG_2024', 'SchneiderElectric_2024', 'Philips_2024']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55ae25b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              report_name query  \\\n",
      "0      ContinentalAG_2024  S1_E   \n",
      "1  SchneiderElectric_2024  S1_E   \n",
      "2      ContinentalAG_2024  S1_D   \n",
      "3      ContinentalAG_2024  S1_A   \n",
      "4  SchneiderElectric_2024  S1_F   \n",
      "\n",
      "                                                text page_number  \n",
      "0  In accordance with Section 76 (4) AktG, the Ex...          27  \n",
      "1  Our 2025 sustainability commitments\\nWith less...          33  \n",
      "2  The globally applicable Code of\\nConduct provi...          41  \n",
      "3  The globally applicable Code of\\nConduct provi...          41  \n",
      "4  Our Speak Up Mindset\\nSchneider Electric emplo...          41  \n"
     ]
    }
   ],
   "source": [
    "# Read in the manually hand coded validation set (based on the sample reports)\n",
    "validation_set = pd.read_excel('validation_dataset.xlsx')\n",
    "print(validation_set.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4072f477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query  report_name           \n",
      "S1_A   ContinentalAG_2024        22\n",
      "       Philips_2024              13\n",
      "       SchneiderElectric_2024    24\n",
      "S1_B   ContinentalAG_2024        14\n",
      "       Philips_2024               4\n",
      "       SchneiderElectric_2024     4\n",
      "S1_C   ContinentalAG_2024        19\n",
      "       Philips_2024              13\n",
      "       SchneiderElectric_2024    18\n",
      "S1_D   ContinentalAG_2024        23\n",
      "       Philips_2024              13\n",
      "       SchneiderElectric_2024    10\n",
      "S1_E   ContinentalAG_2024        12\n",
      "       Philips_2024              11\n",
      "       SchneiderElectric_2024    24\n",
      "S1_F   ContinentalAG_2024        15\n",
      "       Philips_2024               6\n",
      "       SchneiderElectric_2024    23\n",
      "S1_G   ContinentalAG_2024         5\n",
      "       Philips_2024               1\n",
      "       SchneiderElectric_2024     8\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check for consistency in the validation set\n",
    "print(validation_set.groupby(['query', 'report_name' ]).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d4f812",
   "metadata": {},
   "source": [
    "# 0: Baseline approach\n",
    "Based on Ni et al., 2023 and Colesanti Senni et al., 2025\n",
    "    Ni, J., Bingler, J., Colesanti-Senni, C., Kraus, M., Gostlow, G., Schimanski, T., Stammbach, D., Vaghefi, S. A., Wang, Q., Webersinke, N., Wekhof, T., Yu, T., & Leippold, M. (2023). CHATREPORT: Democratizing Sustainability Disclosure Analysis through LLM-based Tools. Swiss Finance Institute Research Paper, No. 23-111. https://doi.org/10.48550/arXiv.2307.15770\n",
    "    Colesanti Senni, C., Schimanski, T., Bingler, J., Ni, J., & Leippold, M. (2025). Using AI to assess corporate climate transition disclosures. Environmental Research Communications, 7(2), 021010. https://doi.org/10.1088/2515-7620/ad9e88\n",
    "\n",
    "With the configuration:\n",
    "- PDF text extraction: MyPuPDF\n",
    "- Retrieval:\n",
    "    - Embedding model: OpenAI text-embedding-ada-002\n",
    "    - top k: 8\n",
    "    - chunk size: 350\n",
    "    - chunk overlap: 50\n",
    "- Generation:\n",
    "    - Generative LLM: OpenAI o4-mini-2025-04-16\n",
    "    - answer_length: 200\n",
    "    - temperature: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e325a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieval settings\n",
    "TOP_K = 8\n",
    "CHUNK_SIZE = 350\n",
    "CHUNK_OVERLAP = 50\n",
    "\n",
    "# generation settings\n",
    "llm_name = 'o4-mini-2025-04-16'\n",
    "max_token=200\n",
    "\n",
    "\n",
    "QUERIES = {\n",
    "    'S1_A': \"How does the company manage and disclose material impacts, risks and opportunities related to the own workforce?\",\n",
    "    'S1_B': \"What are the material risks and opportunities arising from the company’s impacts and dependencies on people in its own workforce?\",\n",
    "    'S1_C': \"What are the company’s human rights practices, risks and incidents related to the own workforce?\",\n",
    "    'S1_D': \"What are the company’s processes and policies for engaging with own workers and workers’ representatives about impacts?\",\n",
    "    'S1_E': \"What are the company’s policies on non-discrimination, diversity and inclusion in the own workforce?\",\n",
    "    'S1_F': \"What are the company’s processes, policies and approaches to remediate negative impacts and channels for own workers to raise concerns?\",\n",
    "    'S1_G': \"How is the company’s workfoce social protection coverage?\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dd733e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### HELPER FUNCTIONS ###\n",
    "\n",
    "# preparing filenames\n",
    "def prepare_filename(name):\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"\", name)\n",
    "\n",
    "# for the generation\n",
    "def remove_brackets(string):\n",
    "    return re.sub(r'\\([^)]*\\)', '', string).strip()\n",
    "\n",
    "def _docs_to_string(docs, with_source=True):\n",
    "# def _docs_to_string(docs, num_docs=TOP_K, with_source=True):\n",
    "    output = \"\"\n",
    "    # docs = docs[:num_docs]\n",
    "    for doc in docs:\n",
    "        output += \"Content: {}\\n\".format(doc.page_content)\n",
    "        if with_source:\n",
    "            output += \"Source: {}\\n\".format(doc.metadata['source'])\n",
    "        output += \"\\n---\\n\"\n",
    "    return output\n",
    "\n",
    "def _find_answer(string, name=\"ANSWER\"):\n",
    "    for l in string.split('\\n'):\n",
    "        if name in l:\n",
    "            start = l.find(\":\") + 3\n",
    "            end = len(l) - 1\n",
    "            return l[start:end]\n",
    "    return string\n",
    "\n",
    "def _find_sources(string):\n",
    "    pattern = r'\\d+'\n",
    "    numbers = [int(n) for n in re.findall(pattern, string)]\n",
    "    return numbers\n",
    "\n",
    "def _find_float_numbers(string):\n",
    "    pattern = r\"[-+]?[0-9]*\\.?[0-9]+([eE][-+]?[0-9]+)?\"\n",
    "    float_numbers = [float(n) for n in re.findall(pattern, string)]\n",
    "    return float_numbers\n",
    "\n",
    "def _find_score(string):\n",
    "    for l in string.split('\\n'):\n",
    "        if \"SCORE\" in l:\n",
    "            d = re.search(r'[-+]?\\d*\\.?\\d+', l)\n",
    "            break\n",
    "    return d[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92e0137c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### RETRIEVAL ###\n",
    "\n",
    "# 1. Load the PDF\n",
    "def load_pdf(path=None, url=None):\n",
    "    assert (path is not None) != (url is not None), \"Either path or url must be provided\"\n",
    "    \n",
    "    if path:\n",
    "        return pymupdf.open(path)\n",
    "    else:\n",
    "        response = requests.get(url)\n",
    "        pdf_bytes = io.BytesIO(response.content)\n",
    "        return pymupdf.open(stream=pdf_bytes, filetype='pdf')\n",
    "    \n",
    "# 2. Extract text from the PDF\n",
    "def extract_text(pdf):\n",
    "    text_list = [page.get_text() for page in pdf]\n",
    "    all_text = ''.join(text_list)\n",
    "    return text_list, all_text\n",
    "\n",
    "# 4. Create or Load Vector Store\n",
    "def get_retriever(pdf, db_path, top_k=TOP_K, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP):\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \"],\n",
    "    )\n",
    "\n",
    "    chunks = []\n",
    "    page_idx = []\n",
    "\n",
    "    for i, page in enumerate(pdf):\n",
    "        page_chunks = text_splitter.split_text(page.get_text())\n",
    "        page_idx.extend([i + 1] * len(page_chunks))\n",
    "        chunks.extend(page_chunks)\n",
    "\n",
    "    if os.path.exists(db_path):\n",
    "        doc_search = FAISS.load_local(db_path, embeddings=embeddings, allow_dangerous_deserialization=True)\n",
    "    else:\n",
    "        doc_search = FAISS.from_texts(\n",
    "            chunks,\n",
    "            embeddings,\n",
    "            metadatas=[{\"source\": str(i), \"page\": str(idx)} for i, idx in enumerate(page_idx)]\n",
    "        )\n",
    "        doc_search.save_local(db_path)\n",
    "\n",
    "    retriever = doc_search.as_retriever(search_kwargs={\"k\": top_k})\n",
    "    return retriever, doc_search\n",
    "\n",
    "# 5. Retrieve relevant chunks\n",
    "def retrieve_chunks(retriever, queries):\n",
    "    section_text_dict = {}\n",
    "\n",
    "    for key, prompts in queries.items():\n",
    "        if key == 'general' and isinstance(prompts, list):\n",
    "            combined_docs = []\n",
    "            for prompt in prompts:\n",
    "                combined_docs.extend(retriever.invoke(prompt)[:5])\n",
    "            section_text_dict[key] = combined_docs\n",
    "        else:\n",
    "            section_text_dict[key] = retriever.invoke(prompts)\n",
    "    \n",
    "    return section_text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85bf68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: ContinentalAG_2024\n",
      "\n",
      "Processing: SchneiderElectric_2024\n",
      "\n",
      "Processing: Philips_2024\n"
     ]
    }
   ],
   "source": [
    "# Apply retrieval the functions to the 3 sampled reports\n",
    "all_results = {}\n",
    "\n",
    "for idx, row in sample.iterrows():\n",
    "    filename = f\"{prepare_filename(row['company_withAccessInfo'])}_2024\"\n",
    "    filename = filename.replace(\" \", \"\")\n",
    "    print(f\"\\nProcessing: {filename}\")\n",
    "\n",
    "    try:\n",
    "        PATH = f\"./sample_reports/{filename}.pdf\"\n",
    "        pdf = load_pdf(path=PATH) \n",
    "        text_list, all_text = extract_text(pdf)\n",
    "        DB_PATH = f\"./faiss_db_OpenAI_0/{filename}\"\n",
    "        retriever, doc_search = get_retriever(pdf, db_path=DB_PATH)\n",
    "        results = retrieve_chunks(retriever, queries=QUERIES)\n",
    "        all_results[filename] = results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {row['company_withAccessInfo']}: {e}\")\n",
    "\n",
    "# save results\n",
    "results_df = pd.DataFrame.from_dict(all_results, orient='index')\n",
    "results_df.to_csv('retrieval_results_0.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da15c34",
   "metadata": {},
   "source": [
    "*Ressources needed for embeddings of three reports*\n",
    "- Time 1.5 min (50 min for 100 reports)\n",
    "- Costs: $0.2 ($7 for 100 reports)\n",
    "- no GPU needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239c86d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generation ###\n",
    "\n",
    "### 1. Define the Structured Output with Pydantic\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "class ReportAnalysis(BaseModel):\n",
    "    \"\"\"Pydantic model for the structured output of the report analysis.\"\"\"\n",
    "    answer: str = Field(description='The full analysis, starting with \"[[YES]]\" or \"[[NO]]\", followed by a detailed explanation. Max 150 words.')\n",
    "    sources: List[int] = Field(description=\"A list of the integer source numbers referenced in the answer.\")\n",
    "\n",
    "\n",
    "### 2. Create Langchain Prompt Template\n",
    "# 1. Initialize the Pydantic parser based on our model\n",
    "pydantic_parser = PydanticOutputParser(pydantic_object=ReportAnalysis)\n",
    "\n",
    "# 2. Define the prompt template\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are an AI assistant in the role of a Senior Equity Analyst with expertise in sustainability reporting that analyzes companys' annual reports.\"),\n",
    "        (\"human\", \"\"\"\n",
    "You are a senior sustainabiliy analyst with expertise in sustainability reporting evaluating a company's annual report.\n",
    "\n",
    "You are presented with the following sources from the company's report. Each source is numbered.\n",
    "--------------------- [BEGIN OF SOURCES]\n",
    "{context}\n",
    "--------------------- [END OF SOURCES]\n",
    "\n",
    "Given the sources information and no prior knowledge, your main task is to respond to the posed question encapsulated in \"||\".\n",
    "Question: ||{question}||\n",
    "\n",
    "Please consider the following additional explanation to the question:\n",
    "+++++ [BEGIN OF EXPLANATION]\n",
    "{explanation}\n",
    "+++++ [END OF EXPLANATION]\n",
    "\n",
    "Please enforce to the following guidelines in your answer:\n",
    "1. Your response must be precise and grounded on specific extracts from the report.\n",
    "2. If you are unsure, simply acknowledge it.\n",
    "3. Keep your answer within 150 words.\n",
    "4. Be skeptical and critical of the information disclosed.\n",
    "5. Identify and be critical of any \"cheap talk\" (costless, unverifiable statements).\n",
    "6. Scrutinize whether the report provides quantifiable data versus vague statements.\n",
    "7. Start your answer with \"[[YES]]\" or \"[[NO]]\", followed by a short, informative explanation.\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 3. Inject the auto-generated formatting instructions into the prompt\n",
    "partial_prompt = prompt_template.partial(\n",
    "    format_instructions=pydantic_parser.get_format_instructions()\n",
    ")\n",
    "\n",
    "\n",
    "### 3. Core "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed46eaee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bbf7019",
   "metadata": {},
   "source": [
    "# A) Embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6079b8f6",
   "metadata": {},
   "source": [
    "# B) Retrieval settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd67d692",
   "metadata": {},
   "source": [
    "# C) Retrieval queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f638c306",
   "metadata": {},
   "source": [
    "# D) Generative LLM Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacd314b",
   "metadata": {},
   "source": [
    "# E) PDF Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf76355",
   "metadata": {},
   "source": [
    "# F) Generation prompts\n",
    "- include \"basic information\" \n",
    "This is basic information to the company:\n",
    "{basic_info}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d4ffca",
   "metadata": {},
   "source": [
    "# Evaluation: System soley based on Ni et al. 2023\n",
    "- goal LLM directly outputs a conformity score [0, 100] on the overarching themes S1_A - S1_G\n",
    "- hand coded a ground-truth dataset on the retrieval chunks for every indicator in the three sample reports\n",
    "\n",
    "- PDF text extraction: MyPuPDF\n",
    "- Retrieval:\n",
    "    - Embedding model: OpenAI text-embedding-ada-002\n",
    "    - top k: 20\n",
    "    - chunk size: 500\n",
    "    - chunk overlap: 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd306221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              report_name query  \\\n",
      "0      ContinentalAG_2024  S1_E   \n",
      "1  SchneiderElectric_2024  S1_E   \n",
      "2      ContinentalAG_2024  S1_D   \n",
      "3      ContinentalAG_2024  S1_A   \n",
      "4  SchneiderElectric_2024  S1_F   \n",
      "\n",
      "                                                text page_number  \n",
      "0  In accordance with Section 76 (4) AktG, the Ex...          27  \n",
      "1  Our 2025 sustainability commitments\\nWith less...          33  \n",
      "2  The globally applicable Code of\\nConduct provi...          41  \n",
      "3  The globally applicable Code of\\nConduct provi...          41  \n",
      "4  Our Speak Up Mindset\\nSchneider Electric emplo...          41  \n"
     ]
    }
   ],
   "source": [
    "validation_set = pd.read_excel('old_validation_dataset.xlsx', sheet_name='S1_Retrieval')\n",
    "print(validation_set.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "659ce3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query  report_name           \n",
      "S1_A   ContinentalAG_2024        22\n",
      "       Philips_2024              13\n",
      "       SchneiderElectric_2024    24\n",
      "S1_B   ContinentalAG_2024        14\n",
      "       Philips_2024               4\n",
      "       SchneiderElectric_2024     4\n",
      "S1_C   ContinentalAG_2024        19\n",
      "       Philips_2024              13\n",
      "       SchneiderElectric_2024    18\n",
      "S1_D   ContinentalAG_2024        23\n",
      "       Philips_2024              13\n",
      "       SchneiderElectric_2024    10\n",
      "S1_E   ContinentalAG_2024        12\n",
      "       Philips_2024              11\n",
      "       SchneiderElectric_2024    24\n",
      "S1_F   ContinentalAG_2024        15\n",
      "       Philips_2024               6\n",
      "       SchneiderElectric_2024    23\n",
      "S1_G   ContinentalAG_2024         5\n",
      "       Philips_2024               1\n",
      "       SchneiderElectric_2024     8\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(validation_set.groupby(['query', 'report_name' ]).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a047cf1",
   "metadata": {},
   "source": [
    "## Evaluate retrieval performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6863b547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'It is in line with the principle of equal pay for equal work.'}\n"
     ]
    }
   ],
   "source": [
    "ground_truth_sentences = set(['Equal pay /nFair and equitable pay is a core component of the Group’s compensation philosophy.',\n",
    "                            'It is in line with the principle of equal pay for equal work.',\n",
    "                            '100 % of Schneider are paid /nat least a living wage, which /nwas recognized for the /nsecond consecutive year by /nthe Living Wage Employer /nCertification from Fair /nWage Network.'])\n",
    "retrieved_sentences = set(['Fair and equitable pay is a core component of the Group’s compensation philosophy.',\n",
    "                           'It is in line with the principle of equal pay for equal work.',\n",
    "                           '100 % of Schneider are paid /nat least a living wage, which /nwas recognized for the /nsecond consecutive year'])\n",
    "\n",
    "found_ground_truth_sentences = set()\n",
    "for sentence in retrieved_sentences:\n",
    "    if sentence in ground_truth_sentences:\n",
    "        found_ground_truth_sentences.add(sentence)\n",
    "\n",
    "print(found_ground_truth_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f88a17",
   "metadata": {},
   "source": [
    "This approach is very restrictive and cause for lower scores whenever the cunk size abbreviates sentences, eventhough the LLM could still understand the meaning.\n",
    "\n",
    "2. Approach fuzzy string matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2615ad1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 93, Retrieved sentence: 'Fair and equitable pay is a core component of the Group’s compensation philosophy.', Found match: 'Equal pay /nFair and equitable pay is a core component of the Group’s compensation philosophy.'\n",
      "Score: 100, Retrieved sentence: 'It is in line with the principle of equal pay for equal work.', Found match: 'It is in line with the principle of equal pay for equal work.'\n"
     ]
    }
   ],
   "source": [
    "found_matches = 0\n",
    "for sentence in retrieved_sentences:\n",
    "    best_match, score = process.extractOne(sentence, ground_truth_sentences, scorer=fuzz.ratio)\n",
    "    if score > 80:\n",
    "        found_matches += 1\n",
    "        print(f\"Score: {score}, Retrieved sentence: '{sentence}', Found match: '{best_match}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "155af939",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval_sentence_level(retrieved_docs, ground_truth_texts):\n",
    "    \"\"\"\n",
    "    Evaluates retrieval performance on a sentence level using fuzzy string matching.\n",
    "\n",
    "    Args:\n",
    "        retrieved_docs (list): A list of Document objects retrieved by LangChain.\n",
    "        ground_truth_texts (list): A list of ground-truth text snippets from the validation set.\n",
    "        score_threshold (int): The similarity score (0-100) required to consider a sentence a match.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing precision, recall, and f1-score.\n",
    "    \"\"\"\n",
    "    # 1. Extract all ground-truth and retrieved sentences\n",
    "    all_ground_truth_sentences = set()\n",
    "    for text in ground_truth_texts:\n",
    "        sentences = sent_tokenize(text)\n",
    "        all_ground_truth_sentences.update([s.strip() for s in sentences if s.strip()])\n",
    "\n",
    "    if not all_ground_truth_sentences:\n",
    "        return {\"precision\": None, \"recall\": None, \"f1\": None}\n",
    "\n",
    "    all_retrieved_sentences = set()\n",
    "    for doc in retrieved_docs:\n",
    "        chunk_sentences = sent_tokenize(doc.page_content)\n",
    "        all_retrieved_sentences.update([s.strip() for s in chunk_sentences if s.strip()])\n",
    "\n",
    "    if not all_retrieved_sentences:\n",
    "        return {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
    "    \n",
    "    # 2. For each retrieved sentence, find its best match in the ground truth sentences.\n",
    "    found_matches = 0\n",
    "    for retrieved_sentence in all_retrieved_sentences:\n",
    "        # process.extractOne finds the best matching string from a collection.\n",
    "        # It returns a tuple: (best_match_string, score)\n",
    "        best_match, score = process.extractOne(\n",
    "            retrieved_sentence, \n",
    "            all_ground_truth_sentences, \n",
    "            scorer=fuzz.ratio\n",
    "        )\n",
    "        \n",
    "        # If the best match has a score above our threshold, we count it as a successful find.\n",
    "        if score >= 80:\n",
    "            found_matches += 1\n",
    "\n",
    "    # 3. Calculate metrics based on the fuzzy matches.\n",
    "    true_positives = found_matches\n",
    "    \n",
    "    # Precision = (Relevant sentences found) / (Total sentences retrieved)\n",
    "    precision = true_positives / len(all_retrieved_sentences) if all_retrieved_sentences else 0.0\n",
    "    \n",
    "    # Recall = (Relevant sentences found) / (Total relevant sentences that exist)\n",
    "    recall = true_positives / len(all_ground_truth_sentences) if all_ground_truth_sentences else 0.0\n",
    "    \n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "\n",
    "def evaluate_retrieval(retrieval_results):\n",
    "    \"\"\"\n",
    "    Evaluates the retrieval performance of a LangChain vector store against a validation set.\n",
    "\n",
    "    Args:\n",
    "        retrieval_results (list): A list of Document objects retrieved by LangChain.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing precision, recall, and f1-score.\n",
    "    \"\"\"\n",
    "    evaluation_results_all = []\n",
    "    \n",
    "    for report_name, queries_results in retrieval_results.items():\n",
    "        for query_key, retrieved_documents in queries_results.items():\n",
    "\n",
    "            # Get the corresponding ground-truth texts from the validation set\n",
    "            gt_texts = validation_set[\n",
    "                (validation_set['report_name'] == report_name) & \n",
    "                (validation_set['query'] == query_key)\n",
    "            ]['text'].tolist()\n",
    "\n",
    "            if not gt_texts:\n",
    "                continue\n",
    "\n",
    "            # Evaluate the retrieval performance\n",
    "            scores = evaluate_retrieval_sentence_level(\n",
    "                retrieved_docs=retrieved_documents,\n",
    "                ground_truth_texts=gt_texts\n",
    "            )\n",
    "\n",
    "            evaluation_results_all.append({\n",
    "                \"report_name\": report_name,\n",
    "                \"query\": query_key,\n",
    "                **scores\n",
    "            })\n",
    "\n",
    "    evaluation_results_mean = pd.DataFrame(evaluation_results_all).groupby(['query'])[['precision', 'recall', 'f1']].mean().reset_index()\n",
    "    overall_mean = evaluation_results_mean[['precision', 'recall', 'f1']].mean()\n",
    "    evaluation_results_mean.loc['Overall Mean'] = overall_mean\n",
    "\n",
    "    print(\"--- Retrieval Performance Summary ---\")\n",
    "    print(evaluation_results_mean.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3fbb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on Ni et al. 2023\n",
    "TOP_K = 20\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 20\n",
    "\n",
    "QUERIES = {\n",
    "    'S1_A': \"How does the company manage and disclose material impacts, risks and opportunities related to the own workforce?\",\n",
    "    'S1_B': \"What are the material risks and opportunities arising from the company’s impacts and dependencies on people in its own workforce?\",\n",
    "    'S1_C': \"What are the company’s human rights practices, risks and incidents related to the own workforce?\",\n",
    "    'S1_D': \"What are the company’s processes and policies for engaging with own workers and workers’ representatives about impacts?\",\n",
    "    'S1_E': \"What are the company’s policies on non-discrimination, diversity and inclusion in the own workforce?\",\n",
    "    'S1_F': \"What are the company’s processes, policies and approaches to remediate negative impacts and channels for own workers to raise concerns?\",\n",
    "    'S1_G': \"How is the company’s workfoce social protection coverage?\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6cd77c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing: ContinentalAG_2024\n",
      "\n",
      "Processing: SchneiderElectric_2024\n",
      "\n",
      "Processing: Philips_2024\n",
      "--- Retrieval Performance Summary ---\n",
      "             query  precision  recall     f1\n",
      "0             S1_A      0.479   0.028  0.052\n",
      "1             S1_B      0.156   0.088  0.109\n",
      "2             S1_C      0.216   0.058  0.091\n",
      "3             S1_D      0.214   0.043  0.072\n",
      "4             S1_E      0.549   0.135  0.215\n",
      "5             S1_F      0.212   0.044  0.072\n",
      "6             S1_G      0.235   0.250  0.235\n",
      "Overall Mean   NaN      0.295   0.092  0.121\n"
     ]
    }
   ],
   "source": [
    "# retrive relevant chunks from the sample reports\n",
    "all_results = {}\n",
    "\n",
    "for idx, row in sample.iterrows():\n",
    "    filename = f\"{prepare_filename(row['company_withAccessInfo'])}_2024\"\n",
    "    filename = filename.replace(\" \", \"\")\n",
    "    print(f\"\\nProcessing: {filename}\")\n",
    "\n",
    "    try:\n",
    "        PATH = f\"./sample_reports/{filename}.pdf\"\n",
    "        pdf = load_pdf(path=PATH) \n",
    "        text_list, all_text = extract_text(pdf)\n",
    "        DB_PATH = f\"./faiss_db_OpenAI_0/{filename}\"\n",
    "        retriever, doc_search = get_retriever(pdf, db_path=DB_PATH)\n",
    "        results = retrieve_chunks(retriever, queries=QUERIES)\n",
    "        all_results[filename] = results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {row['company_withAccessInfo']}: {e}\")\n",
    "\n",
    "evaluate_retrieval(all_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e6e85c",
   "metadata": {},
   "source": [
    "We observe that the Retrival Recall is very low. Only 9% of all sentences that were manually evaluated as important for the indicator were included in the retrieved sentences from the system.\n",
    "Additionally, only 30% of the sentences the model retrieved were considered important."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
