{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bad62f1c-6b7a-43ab-b6b9-1ce6f1f41fd9",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1351bc2-7fcf-4eb5-ac80-c43f42052328",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tu/tu_tu/tu_zxowg46/thesis/myEnv_thesis/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import re\n",
    "import urllib3\n",
    "import tenacity\n",
    "import configparser\n",
    "import markdown\n",
    "import json\n",
    "import pymupdf\n",
    "import requests\n",
    "import os\n",
    "import io\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import tiktoken\n",
    "import asyncio\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from torch import cuda, bfloat16\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "from torch import cuda, bfloat16\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "from math import ceil\n",
    "from datasets import Dataset\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
    "import pymupdf4llm\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "dotenv_path = os.path.expanduser(\"~/thesis/esg_extraction/.env\")\n",
    "load_dotenv(dotenv_path=dotenv_path)\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# for working with huggingface\n",
    "from huggingface_hub import login\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "login(HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046974e3-2cd7-4966-b386-102cda7939b0",
   "metadata": {},
   "source": [
    "## Validation approach\n",
    "Use sample reports to manually hand code the \"ground truth\" that the output of the LLM is compared against\n",
    "Source: Sustainability Reporting Navigator (crowd-source list of CSRD-compliant reports for fiscal years starting on 01/01/2024)\n",
    "\n",
    "Downloaded CSV with information on all reports on the 08/04/2025 https://www.sustainabilityreportingnavigator.com/#/csrdreports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bfb11db-6b22-498e-a7a5-2035ba86280f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>company_withAccessInfo</th>\n",
       "      <th>link</th>\n",
       "      <th>country</th>\n",
       "      <th>sector</th>\n",
       "      <th>industry</th>\n",
       "      <th>publication date</th>\n",
       "      <th>pages PDF</th>\n",
       "      <th>auditor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>266</td>\n",
       "      <td>Schneider Electric*</td>\n",
       "      <td>https://www.se.com/ww/en/assets/564/document/5...</td>\n",
       "      <td>France</td>\n",
       "      <td>Resource Transformation</td>\n",
       "      <td>Electrical &amp; Electronic Equipment</td>\n",
       "      <td>2025-03-26</td>\n",
       "      <td>186</td>\n",
       "      <td>PwC &amp; Mazars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>19</td>\n",
       "      <td>Continental AG</td>\n",
       "      <td>https://annualreport.continental.com/2024/en/s...</td>\n",
       "      <td>Germany</td>\n",
       "      <td>Transportation</td>\n",
       "      <td>Auto Parts</td>\n",
       "      <td>2025-03-18</td>\n",
       "      <td>125</td>\n",
       "      <td>PwC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0 company_withAccessInfo  \\\n",
       "58          266    Schneider Electric*   \n",
       "220          19         Continental AG   \n",
       "\n",
       "                                                  link  country  \\\n",
       "58   https://www.se.com/ww/en/assets/564/document/5...   France   \n",
       "220  https://annualreport.continental.com/2024/en/s...  Germany   \n",
       "\n",
       "                      sector                           industry  \\\n",
       "58   Resource Transformation  Electrical & Electronic Equipment   \n",
       "220           Transportation                         Auto Parts   \n",
       "\n",
       "    publication date  pages PDF       auditor  \n",
       "58        2025-03-26        186  PwC & Mazars  \n",
       "220       2025-03-18        125           PwC  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reports_24 = pd.read_csv('esg_reports_2024.csv')\n",
    "sample = reports_24[reports_24['company_withAccessInfo'].isin(['Continental AG', 'Schneider Electric*'])]\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "213875b8-acf7-43d5-ae1b-4c330c35f726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>report_name</th>\n",
       "      <th>query</th>\n",
       "      <th>verdict</th>\n",
       "      <th>analysis</th>\n",
       "      <th>sources</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ContinentalAG_2024</td>\n",
       "      <td>S1_A1</td>\n",
       "      <td>YES</td>\n",
       "      <td>[[YES]] \\n\"The consolidation at group level of...</td>\n",
       "      <td>[175, 176]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ContinentalAG_2024</td>\n",
       "      <td>S1_A2</td>\n",
       "      <td>YES</td>\n",
       "      <td>[[YES]] \\nreport explicitly mentions character...</td>\n",
       "      <td>[175, 176]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ContinentalAG_2024</td>\n",
       "      <td>S1_A3</td>\n",
       "      <td>NO</td>\n",
       "      <td>[[NO]] \\ndoes not describe the types of non-em...</td>\n",
       "      <td>[176]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ContinentalAG_2024</td>\n",
       "      <td>S1_A4</td>\n",
       "      <td>YES</td>\n",
       "      <td>[[YES]]\\nworking time arrangements, paying ade...</td>\n",
       "      <td>[110, 173, 174]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ContinentalAG_2024</td>\n",
       "      <td>S1_A5</td>\n",
       "      <td>NO</td>\n",
       "      <td>[[NO]]\\ndoes not explicitly describe the types...</td>\n",
       "      <td>[173, 174]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          report_name  query verdict  \\\n",
       "0  ContinentalAG_2024  S1_A1     YES   \n",
       "1  ContinentalAG_2024  S1_A2     YES   \n",
       "2  ContinentalAG_2024  S1_A3      NO   \n",
       "3  ContinentalAG_2024  S1_A4     YES   \n",
       "4  ContinentalAG_2024  S1_A5      NO   \n",
       "\n",
       "                                            analysis          sources  \n",
       "0  [[YES]] \\n\"The consolidation at group level of...       [175, 176]  \n",
       "1  [[YES]] \\nreport explicitly mentions character...       [175, 176]  \n",
       "2  [[NO]] \\ndoes not describe the types of non-em...            [176]  \n",
       "3  [[YES]]\\nworking time arrangements, paying ade...  [110, 173, 174]  \n",
       "4  [[NO]]\\ndoes not explicitly describe the types...       [173, 174]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the manually hand coded validation set (based on the sample reports)\n",
    "validation_set = pd.read_excel('validation_dataset.xlsx')\n",
    "validation_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95e997df-8f5c-444b-836e-e3fb9927fbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate the performance of the YES/NO predictions\n",
    "def evaluate_verdicts(df_val, predicted_results, verbose=False):\n",
    "    print(\"\\n--- LLM Evaluation: Evaluating performance on verdicts ---\")\n",
    "    y_true, y_pred = [], []\n",
    "    mismatches = []\n",
    "    dropped = 0\n",
    "    total = len(df_val)\n",
    "\n",
    "    for _, row in df_val.iterrows():\n",
    "        report = row[\"report_name\"]\n",
    "        query = row[\"query\"]\n",
    "        true_verdict = row[\"verdict\"]\n",
    "        pred_verdict = predicted_results.get(report, {}).get(query, {}).get(\"verdict\")\n",
    "        true_analysis = row[\"analysis\"]\n",
    "        pred_analysis = predicted_results.get(report, {}).get(query, {}).get(\"analysis\")\n",
    "\n",
    "        # Drop if either is missing or N/A\n",
    "        if pred_verdict is None or pred_verdict == \"N/A\":\n",
    "            if verbose:\n",
    "                print(f\"[DROPPED] {report} | {query} | True verdict: {true_verdict} | Predicted verdict: {pred_verdict}, LLM Analysis: {analysis}\")\n",
    "            dropped += 1\n",
    "            continue\n",
    "\n",
    "        y_true.append(true_verdict)\n",
    "        y_pred.append(pred_verdict)\n",
    "\n",
    "        if verbose and true_verdict != pred_verdict:\n",
    "            mismatches.append((report, query, true_verdict, pred_verdict, true_analysis, pred_analysis))\n",
    "\n",
    "    if verbose and mismatches:\n",
    "        print(\"\\n Mismatches:\")\n",
    "        for report, query, true, pred, true_analysis, pred_analysis in mismatches:\n",
    "            print(f\"  {report} | {query} \")\n",
    "            print(f\" TRUE VERDICT: {true}, TRUE ANALYSIS   : {true_analysis}\")\n",
    "            print(f\" PRED VERDICT: {pred}, PRED ANALYSIS   : {pred_analysis}\")\n",
    "\n",
    "    print(f\"\\n Dropped {dropped} of {total} queries due to missing verdicts.\")\n",
    "\n",
    "    # Check to avoid errors when nothing is left\n",
    "    if not y_true:\n",
    "        print(\" No valid data to evaluate.\")\n",
    "        return {\n",
    "            \"accuracy\": None,\n",
    "            \"precision\": None,\n",
    "            \"recall\": None,\n",
    "            \"f1_score\": None\n",
    "        }\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='weighted', zero_division=0\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6109c46f-a12c-4fb5-9f4c-1ac4a7a8b835",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_results = []\n",
    "\n",
    "def evaluate_verdicts(df_val, predicted_results, config_name, llm_name, embedding_model, resources, verbose=False):\n",
    "    print(\"\\n--- LLM Evaluation: Evaluating performance on verdicts ---\")\n",
    "    y_true, y_pred = [], []\n",
    "    mismatches = []\n",
    "    dropped = 0\n",
    "    total = len(df_val)\n",
    "\n",
    "    for _, row in df_val.iterrows():\n",
    "        report = row[\"report_name\"]\n",
    "        query = row[\"query\"]\n",
    "        true_verdict = row[\"verdict\"]\n",
    "        pred_verdict = predicted_results.get(report, {}).get(query, {}).get(\"verdict\")\n",
    "        true_analysis = row[\"analysis\"]\n",
    "        pred_analysis = predicted_results.get(report, {}).get(query, {}).get(\"analysis\")\n",
    "\n",
    "        if pred_verdict is None or pred_verdict == \"N/A\":\n",
    "            dropped += 1\n",
    "            continue\n",
    "\n",
    "        y_true.append(true_verdict)\n",
    "        y_pred.append(pred_verdict)\n",
    "\n",
    "        if verbose and true_verdict != pred_verdict:\n",
    "            mismatches.append((report, query, true_verdict, pred_verdict, true_analysis, pred_analysis))\n",
    "\n",
    "    print(f\"\\n Dropped {dropped} of {total} queries due to missing verdicts.\")\n",
    "\n",
    "    if not y_true:\n",
    "        print(\" No valid data to evaluate.\")\n",
    "        metrics = {\"accuracy\": None, \"precision\": None, \"recall\": None, \"f1_score\": None}\n",
    "    else:\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            y_true, y_pred, average='weighted', zero_division=0\n",
    "        )\n",
    "        metrics = {\n",
    "            \"accuracy\": round(acc, 3),\n",
    "            \"precision\": round(precision, 3),\n",
    "            \"recall\": round(recall, 3),\n",
    "            \"f1_score\": round(f1, 3)\n",
    "        }\n",
    "\n",
    "    # Append to global summary\n",
    "    summary_results.append({\n",
    "        \"Config\": config_name,\n",
    "        \"LLM\": llm_name,\n",
    "        \"Embedding Model\": embedding_model,\n",
    "        \"Accuracy\": metrics[\"accuracy\"],\n",
    "        \"Recall\": metrics[\"recall\"],\n",
    "        \"Precision\": metrics[\"precision\"],\n",
    "        \"F1-Score\": metrics[\"f1_score\"],\n",
    "        \"Resources\": resources\n",
    "    })\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b0dd09-af99-4152-8edf-432ee4d0f593",
   "metadata": {},
   "source": [
    "## Code needed for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "768b30a2-2910-48c7-845b-0f2b738ab154",
   "metadata": {},
   "outputs": [],
   "source": [
    "### HELPER FUNCTIONS ###\n",
    "\n",
    "# preparing filenames\n",
    "def prepare_filename(name):\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"\", name)\n",
    "    \n",
    "\n",
    "def _docs_to_string(docs, with_source=True):\n",
    "    output = \"\"\n",
    "    for doc in docs:\n",
    "        output += \"Content: {}\\n\".format(doc.page_content)\n",
    "        if with_source:\n",
    "            output += \"Source: {}\\n\".format(doc.metadata['page'])\n",
    "        output += \"\\n---\\n\"\n",
    "    return output\n",
    "\n",
    "\n",
    "def _find_answer(full_text):\n",
    "    try:\n",
    "        for line in full_text.splitlines():\n",
    "            if \"ANSWER\" in line:\n",
    "                idx = line.find(\":\") + 1\n",
    "                return line[idx:].strip().strip('\",')\n",
    "        return full_text.strip()  # fallback if no ANSWER found\n",
    "    except Exception:\n",
    "        return full_text.strip()\n",
    "\n",
    "\n",
    "def _find_verdict(answer_text):\n",
    "    if not answer_text:\n",
    "        return \"N/A\"\n",
    "    \n",
    "    # Look for [[YES]] or [[NO]], case-insensitive\n",
    "    match = re.search(r'\\[\\[\\s*(YES|NO)\\s*\\]\\]', answer_text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).upper()\n",
    "    \n",
    "    return \"N/A\"\n",
    "\n",
    "\n",
    "def _find_sources(full_text):\n",
    "    # Attempt to match a SOURCES line first\n",
    "    sources_match = re.search(r\"SOURCES\\s*:\\s*\\[([^\\]]+)\\]\", full_text)\n",
    "    if sources_match:\n",
    "        number_list = re.findall(r'\\d+', sources_match.group(1))\n",
    "        return [int(n) for n in number_list]\n",
    "\n",
    "    # Fallback: extract all numbers from full text\n",
    "    return [int(n) for n in re.findall(r'\\b\\d{3,5}\\b', full_text)]  # assumes sources have 3-5 digits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe875809-72a6-4acd-8874-31caa15613df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  topic_id query_id                                              query  \\\n",
      "0     S1_A    S1_A1  Does the company disclose whether all people i...   \n",
      "1     S1_A    S1_A2  Does the company describe the types of employe...   \n",
      "2     S1_A    S1_A3  Does the company describe the types of non-emp...   \n",
      "\n",
      "                                          guidelines      esrs paragraph  \\\n",
      "0  Focus on whether the company explicitly confir...  S1.SBM-3        14   \n",
      "1  Check whether the company identifies which typ...  S1.SBM-3      14 a   \n",
      "2  Assess whether the company identifies non-empl...  S1.SBM-3      14 a   \n",
      "\n",
      "   related_ar  \n",
      "0  AR 6 - AR7  \n",
      "1         NaN  \n",
      "2         NaN  \n",
      "{'S1_A1': 'Does the company disclose whether all people in its own workforce who could be materially impacted are included in the scope of its disclosure?', 'S1_A2': 'Does the company describe the types of employees in its own workforce that are subject to material impacts?', 'S1_A3': 'Does the company describe the types of non-employees in its own workforce that are subject to material impacts?', 'S1_A4': 'Does the company describe the activities that result in material positive impacts on its own workforce?', 'S1_A5': 'Does the company identify the types of employees and non-employees positively affected or potentially affected by activities that result in material positive impacts on its own workforce?', 'S1_A6': 'Does the company report material impacts on its workforce resulting from transition plans for greener and climate-neutral operations?', 'S1_A7': 'Does the company disclose whether it has developed an understanding of how the people in its own workforce with particular characteristics, contexts, or activities may be at greater risk of harm?', 'S1_A8': 'Does the company disclose policies to manage its material impacts, risks, and opportunities related to its own workforce?', 'S1_A9': 'Does the company specify whether its workforce-related policies apply to specific groups or the entire workforce?', 'S1_A10': 'Does the company describe actions taken, planned or underway to prevent or mitigate material negative impacts on its own workforce?', 'S1_A11': 'Does the company describe how it tracks and assesses the effectiveness of actions and initiatives in delivering outcomes positive impacts or mitigarte negative impacts for its own workforce?', 'S1_A12': 'Does the company describe actions or initiatives that have the primary purpose of delivering positive impacts for its own workforce?', 'S1_A13': 'Does the company describe the processes it uses to identify what action is needed in response to actual or potential negative impacts on its own workforce?', 'S1_A14': 'Does the company describe actions planned or underway to pursue material opportunities related to its own workforce?', 'S1_A15': 'Does the company disclose whether and how it ensures that its own practices do not cause or contribute to material negative impacts on its own workforce?', 'S1_A16': 'Does the company disclose what resources are allocated to the management of its material impacts on its own workforce?', 'S1_B1': 'Does the company disclose any material risks arising from its dependencies on its own workforce?', 'S1_B2': 'Does the company disclose whether any material risks or opportunities related to its workforce apply specifically to certain groups rather than to the entire workforce?', 'S1_B3': 'Does the company describe actions planned or underway to mitigate material risks arising from its impacts and dependencies on its own workforce?', 'S1_B4': 'Does the company describe how it tracks the effectiveness of actions taken to mitigate material risks arising from its impacts and dependencies on its own workforce?', 'S1_C1': 'Does the company disclose operations that are at significant risk of incidents of forced labour or compulsory labour by type of operation (e.g., manufacturing plant)?', 'S1_C2': 'Does the company disclose operations that are at significant risk of incidents of forced labour or compulsory labour by countries or geographic areas?', 'S1_C3': 'Does the company disclose operations that are at significant risk of incidents of child labour by type of operation (e.g., manufacturing plant)?', 'S1_C4': 'Does the company disclose operations that are at significant risk of incidents of child labour by countries or geographic areas?', 'S1_C5': 'Does the company describe its human rights policy commitments relevant to its own workforce?', 'S1_C6': 'Does the company describe its general approach to respect human rights, including labour rights, of people in its own workforce?', 'S1_C7': 'Does the company disclose whether and how its policies with regard to its own workforce are aligned with relevant internationally recognised instruments, including the UN Guiding Principles on Business and Human Rights?', 'S1_C8': 'Does the company state whether its policies related to its own workforce explicitly address trafficking in human beings, forced labour or compulsory labour, and child labour?', 'S1_C9': 'Does the company explain the existence and role of any Global Framework Agreement or other agreements with workers’ representatives related to respecting the human rights of its own workforce?', 'S1_C10': 'Does the company disclose the number of severe human rights incidents connected to its workforce during the reporting period?', 'S1_C11': 'Does the company disclose the total amount of fines, penalties, and compensation paid related to the severe human rights incidents connected to its workforce?', 'S1_D1': 'Does the company disclose its general approach to engage with people in its own workforce?', 'S1_D2': 'Does the company disclose whether and how the perspectives of its own workforce inform its decisions or activities aimed at managing actual and potential impacts on its workforce?', 'S1_D3': 'Does the company disclose whether engagement to inform decisions or activities occurs directly with its own workforce or with workers’ representatives?', 'S1_D4': 'Does the company explain the stage(s) at which engagement with its own workforce occurs?', 'S1_D5': 'Does the company disclose the type of engagement it conducts with its own workforce?', 'S1_D6': 'Does the company disclose the frequency of engagement with its own workforce?', 'S1_D7': 'Does the company disclose the function and the most senior role responsible for ensuring that engagement with its own workforce takes place and informs the company’s decisions or activities?', 'S1_D8': 'Does the company assesses the effectiveness of its engagement with its own workforce to inform its decisions or activities aimed at managing the actual and potential impacts on its own workforce?', 'S1_D9': 'Does the company disclose the steps it takes to gain insight into the perspectives of particularly vulnerable or marginalised people in its own workforce?', 'S1_D10': 'Does the company disclose whether and how it engaged directly with its own workforce or workers’ representatives when setting workforce-related targets?', 'S1_D11': 'Does the company disclose whether and how it engaged directly with its own workforce or workers’ representatives in tracking performance against workforce-related targets?', 'S1_D12': 'Does the company disclose whether and how it engaged directly with its own workforce or workers’ representatives in identifying lessons learned or improvements based on performance against workforce-related targets?', 'S1_E1': 'Does the company report whether it has specific policies aimed at eliminating discrimination, including harassment, and promoting equal opportunities and diversity and inclusion among its workforce?', 'S1_E2': 'Does the company report whether its policy on discrimination among its workforce specifically addresses grounds of discrimination?', 'S1_E3': 'Does the company report whether it has policy commitments that aim to include or support people from groups at particular risk of vulnerability in its workforce?', 'S1_E4': 'Does the company report whether and how its discrimination and inclusion policies for its workforce are implemented?', 'S1_E5': 'Does the company disclose information about reconciliation of fines, penalties, and compensation for damages as a result of work-related discrimination and harassment in its workforce?', 'S1_F1': 'Does the company disclose its general approach to providing and/or enabling remedy for human rights impacts on its own workforce?', 'S1_F2': 'Does the company describe its approach and processes for providing or contributing to remedy for material negative impacts it has caused or contributed to on its own workforce?', 'S1_F3': 'Does the company describe it having specific channels for its own workforce to raise concerns or needs?', 'S1_F4': 'Does the company report having a grievance or complaints handling mechanism for employee matters?', 'S1_F5': 'Does the company describe the processes through which it supports the availability of channels in the workplace of its own workforce to raise concerns or needs?', 'S1_F6': 'Does the company describe how it tracks and monitors issues raised by its own workforce and how they are addressed?', 'S1_F7': 'Does the company describe how it ensures the effectiveness of the channels for its workforce to raise concerns or needs?', 'S1_F8': 'Does the company disclose whether and how it assesses that its own workforce is aware of the channels for raising concerns?', 'S1_F9': 'Does the company disclose whether and how it assesses that its own workforce trusts the channels for raising concerns and having them addressed?', 'S1_F10': 'Does the company disclose whether it has policies in place to protect individuals against retaliation for raising concerns?', 'S1_F11': 'Does the company describe actions it has taken to provide or enable remedy for actual material impacts on its own workforce?', 'S1_G1': 'Does the company disclose whether all its employees are covered by social protection for sickness?', 'S1_G2': 'Does the company disclose whether all its employees are covered by social protection for unemployment starting from when they work for the company?', 'S1_G3': 'Does the company disclose whether all its employees are covered by social protection for employment injury and acquired disability?', 'S1_G4': 'Does the company disclose whether all its employees are covered by social protection for parental leave?', 'S1_G5': 'Does the company disclose whether all its employees are covered by social protection for retirement?', 'S1_G6': 'If not all employees are covered by social protection, does the company disclose the countries where gaps in social protection exist?'}\n",
      "{'S1_A1': 'Focus on whether the company explicitly confirms that all materially impacted individuals in its own workforce are considered. Answer \"YES\" if this is explicitly stated. Answer \"NO\" if the inclusion of all materially impacted individuals is not clearly mentioned.', 'S1_A2': 'Check whether the company identifies which types of employees are affected by material impacts arising from its operations. Answer \"YES\" if employee types are explicitly described. Answer \"NO\" if such types are not specified.', 'S1_A3': 'Assess whether the company identifies non-employees (e.g., self-employed individuals or those provided by third-party agencies) who are materially affected by its operations. Answer \"YES\" if such categories are clearly mentioned. Answer \"NO\" if no non-employee categories are described.', 'S1_A4': 'No additional guidelines', 'S1_A5': 'No additional guidelines', 'S1_A6': 'Impacts, risks and opportunities include restructuring and employment loss as well as opportunities arising from job creation and reskilling or upskilling.', 'S1_A7': 'Focus on whether the company explicitly states that it has developed an understanding of increased risk of harm for specific groups in its own workforce. These include people with particular characteristics (e.g., young people, women, migrants), those working in specific contexts (e.g., poorly regulated labour markets), or performing certain activities (e.g., handling chemicals, zero-hours contracts). Answer \"YES\" if this understanding is disclosed. Answer \"NO\" if no such understanding is mentioned.', 'S1_A8': 'No additional guidelines', 'S1_A9': 'No additional guidelines', 'S1_A10': 'Check whether the company reports on actions that are already implemented, in progress, or planned, which aim to prevent or reduce material negative impacts on its own workforce.', 'S1_A11': 'Focus on whether the company explains how it monitors and evaluates whether its actions and initiatives have effectively addressed material impacts on its own workforce. Tracking and assessment processes may include, but are not limited to, internal or external audits, verification systems, court decisions, impact assessments, measurement systems, stakeholder feedback, grievance mechanisms, benchmarking, or external performance ratings. State \"YES\" if any process for assessing effectiveness is described. State \"NO\" if the report does not include such information.', 'S1_A12': 'No additional guidelines', 'S1_A13': 'No additional guidelines', 'S1_A14': 'No additional guidelines', 'S1_A15': 'Focus on whether the company explains how it ensures that its internal practices — including procurement, sales, or data use, where relevant — do not cause or contribute to material negative impacts on its own workforce. The disclosure may also include how the company handles tensions between preventing such impacts and other business pressures. State \"YES\" if the report explains whether and how the company addresses this. State \"NO\" if there is no such disclosure or the explanation is missing.', 'S1_A16': 'No additional guidelines', 'S1_B1': 'Focus on whether the company reports risks resulting from its dependence on workforce-related conditions. This may include risks of operational disruption due to high employee turnover or lack of skills and training development. ', 'S1_B2': 'Determine whether the company identifies which of its material risks or opportunities related to its own workforce apply to specific groups (e.g., age groups, workers in a particular country or site), instead of applying broadly to the whole workforce (e.g., general pay cut or training offered to all people in its own workforce). State \"YES\" if the company distinguishes group-specific risks or opportunities. State \"NO\" if no such differentiation is made or if the information is only presented at the general workforce level.', 'S1_B3': 'No additional guidelines', 'S1_B4': 'No additional guidelines', 'S1_C1': 'No additional guidelines', 'S1_C2': 'No additional guidelines', 'S1_C3': 'No additional guidelines', 'S1_C4': 'No additional guidelines', 'S1_C5': 'Focus on whether the company discloses its human rights policy commitments specifically related to its own workforce. This includes policies addressing respect for human rights and labour rights, as well as processes and mechanisms to monitor compliance with the UN Guiding Principles on Business and Human Rights, the ILO Declaration on Fundamental Principles and Rights at Work, and the OECD Guidelines for Multinational Enterprises.', 'S1_C6': 'No additional guidelines', 'S1_C7': 'No additional guidelines', 'S1_C8': 'No additional guidelines', 'S1_C9': 'No additional guidelines', 'S1_C10': 'Focus on whether the company discloses the total number of severe human rights incidents, such as forced labour, human trafficking, or child labour, linked to its workforce during the reporting period.', 'S1_C11': 'No additional guidelines', 'S1_D1': 'No additional guidelines', 'S1_D2': 'No additional guidelines', 'S1_D3': 'Focus on whether the company specifies who is involved in the engagement process when informing decisions or activities related to managing actual or potential impacts on its own workforce.', 'S1_D4': 'Focus on whether the company describes at which stage(s) of decision-making or activities the workforce engagement takes place. ', 'S1_D5': 'Focus on the type of engagement of the workforce when informing decisions or activities related to managing actual or potential impacts on its own workforce.', 'S1_D6': 'Focus on the frequency of engagement with its own workforce when informing decisions or activities related to managing actual or potential impacts on its workforce.', 'S1_D7': 'Focus on whether the company specifies both the function and the most senior role within the undertaking that has operational responsibility for ensuring engagement with the workforce occurs and that the results are integrated into the company’s decisions or activities. ', 'S1_D8': 'No additional guidelines', 'S1_D9': 'Focus on whether the company reports any specific actions or procedures it undertakes to understand the perspectives of vulnerable or marginalised groups within its own workforce. These groups may include, but are not limited to, women, migrants, and people with disabilities. ', 'S1_D10': 'Focus on whether the company describes any direct engagement with its own workforce or workers’ representatives specifically during the process of setting workforce-related targets. ', 'S1_D11': 'Focus on whether the company reports involving its workforce or workers’ representatives in the monitoring or tracking of progress toward workforce-related targets.', 'S1_D12': 'Focus on whether the company reports involving its own workforce or workers’ representatives in assessing outcomes or lessons learned from past performance on workforce-related targets, and in identifying improvements.', 'S1_E1': 'No additional guidelines', 'S1_E2': 'Check whether the company specifically addresses any of the following grounds: racial and ethnic origin, colour, sex, sexual orientation, gender identity, disability, age, religion, political opinion, national extraction, social origin, or other forms of discrimination covered by EU or national law.', 'S1_E3': 'No additional guidelines', 'S1_E4': 'Check whether the company discloses specific procedures to ensure discrimination is prevented, mitigated and acted upon once detected, as well as to advance diversity and inclusion in general.', 'S1_E5': 'No additional guidelines', 'S1_F1': 'Focus on whether the company describes any general approach, measures, or processes aimed at providing or enabling remedy for actual or potential human rights impacts affecting its own workforce.', 'S1_F2': 'No additional guidelines', 'S1_F3': 'Focus on whether the company describes the channels available to its own workforce. Examples can be grievance mechanisms, hotlines, trade unions, works councils, or dialogue processes.', 'S1_F4': 'No additional guidelines', 'S1_F5': 'No additional guidelines', 'S1_F6': 'Focus on the process of tracking and monitoring issues from when they are raised to when they are addressed.', 'S1_F7': 'Focus on the processes for ensuring channel effectiveness. This can include the involvement of stakeholders who are the intended users. An example source of information is surveys of people in the undertaking’s workforce that have used such channels and their levels of satisfaction with the process and outcomes.', 'S1_F8': 'Focus on the disclosure of an assessment process regarding workforce awareness of structures like grievance mechanisms or hotlines. The company must state if and how it evaluates whether its own workforce knows about these channels.', 'S1_F9': 'Focus on the disclosure of an assessment process regarding workforce trust. The company must state if and how it evaluates whether its own workforce trusts that these channels are effective and safe to use.', 'S1_F10': 'Focus on the existence of policies protecting individuals, including workers’ representatives, from retaliation when using channels to raise concerns. ', 'S1_F11': \"Focus on the description of actions taken to provide or enable remedy for actual material impacts that have occurred on the company's own workforce.\", 'S1_G1': 'No additional guidelines', 'S1_G2': 'No additional guidelines', 'S1_G3': 'No additional guidelines', 'S1_G4': 'No additional guidelines', 'S1_G5': 'No additional guidelines', 'S1_G6': 'No additional guidelines'}\n"
     ]
    }
   ],
   "source": [
    "# based on the example of Colesanti Senni et al., 2025 defined queries and guidline pairs to capture ESRS guidlines\n",
    "esrs_metadata = pd.read_excel('../EsrsMetadata.xlsx')\n",
    "print(esrs_metadata[:3])\n",
    "\n",
    "# Create QUERIES dictionary\n",
    "QUERIES = dict(zip(esrs_metadata[\"query_id\"], esrs_metadata[\"query\"]))\n",
    "print(QUERIES)\n",
    "\n",
    "# Create GUIDELINES dictionary\n",
    "GUIDELINES = dict(zip(esrs_metadata[\"query_id\"], esrs_metadata[\"guidelines\"]))\n",
    "print(GUIDELINES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c867f002-c055-47af-bc42-573496313a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Parse the Document ##\n",
    "def parse_pdf(path=None, url=None):\n",
    "    assert (path is not None) != (url is not None), \"Provide either a local path or a URL.\"\n",
    "    \n",
    "    if path:\n",
    "        pdf = pymupdf.open(path)\n",
    "    else:\n",
    "        response = requests.get(url)\n",
    "        pdf = pymupdf.open(stream=io.BytesIO(response.content), filetype='pdf')\n",
    "    \n",
    "    pages = [page.get_text() for page in pdf]\n",
    "    full_text = ''.join(pages)\n",
    "    \n",
    "    return pages, full_text\n",
    "\n",
    "\n",
    "## 2. Chunk the text ##\n",
    "def chunk_text(pages, chunk_size, chunk_overlap):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \"],\n",
    "    )\n",
    "\n",
    "    chunks = []\n",
    "    metadata = []\n",
    "\n",
    "    for idx, page in enumerate(pages):\n",
    "        page_chunks = splitter.split_text(page)\n",
    "        chunks.extend(page_chunks)\n",
    "        metadata.extend([{\"page\": str(idx + 1)}] * len(page_chunks))\n",
    "\n",
    "    return chunks, metadata\n",
    "\n",
    "\n",
    "## 3. Generate and store vector representations ##\n",
    "def get_vectorstore(chunks, metadata, db_path, embedding_model):\n",
    "\n",
    "    # if vector representation database already exists load FAISS\n",
    "    if os.path.exists(db_path):\n",
    "        vectorstore = FAISS.load_local(db_path, embeddings=embedding_model, allow_dangerous_deserialization=True)\n",
    "    else:\n",
    "        vectorstore = FAISS.from_texts(chunks, embedding_model, metadatas=metadata)\n",
    "        vectorstore.save_local(db_path)\n",
    "\n",
    "    return vectorstore\n",
    "\n",
    "\n",
    "## 4. Retrieve relevant chunks ##\n",
    "def retrieve_chunks(vectorstore, queries, report_id, top_k):\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": top_k})\n",
    "    \n",
    "    section_text_dict = {}\n",
    "\n",
    "    for key, query in queries.items():\n",
    "        section_text_dict[key] = retriever.invoke(query)\n",
    "    \n",
    "    return {report_id: section_text_dict}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88adf9a7-f604-4559-b496-5d8a33146493",
   "metadata": {},
   "source": [
    "# Baseline Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "770a577a-da28-4684-ad92-13d76fb1c458",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_K = 8\n",
    "CHUNK_SIZE = 350\n",
    "CHUNK_OVERLAP = 50\n",
    "ANSWER_LENGTH=200\n",
    "MAX_TOKEN=500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d494b6-b4c1-42a9-b538-e524d48cb677",
   "metadata": {},
   "source": [
    "## 1. OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2b30b6b-b623-40a0-84c3-bb46bcf7bfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = (\"\"\"\n",
    "You are a senior sustainabiliy analyst with expertise in the european reporting standards evaluating a company's disclosure on social sustainability.\n",
    "\n",
    "\n",
    "You are presented with the following sources from the company's annual report:\n",
    "--------------------- [BEGIN OF SOURCES]\\n\n",
    "{sources}\\n\n",
    "--------------------- [END OF SOURCES]\\n\n",
    "\n",
    "Given the sources information and no prior knowledge, your main task is to respond to the posed question encapsulated in \"||\".\n",
    "Question: ||{query}||\n",
    "\n",
    "Please consider the following additional explanation to the question encapsulated in \"+++++\" as crucial for answering the question:\n",
    "+++++ [BEGIN OF EXPLANATION]\n",
    "{guideline}\n",
    "+++++ [END OF EXPLANATION]\n",
    "\n",
    "Please enforce to the following guidelines in your answer:\n",
    "1. Your response must be precise, thorough, and grounded on specific extracts from the report to verify its authenticity.\n",
    "2. If you are unsure, simply acknowledge the lack of knowledge, rather than fabricating an answer.\n",
    "3. Keep your ANSWER within {answer_length} words.\n",
    "4. Be skeptical to the information disclosed in the report as there might be greenwashing (exaggerating the firm's environmental responsibility). Always answer in a critical tone.\n",
    "5. Cheap talks are statements that are costless to make and may not necessarily reflect the true intentions or future actions of the company. Be critical for all cheap talks you discovered in the report.\n",
    "6. Always acknowledge that the information provided is representing the company's view based on its report.\n",
    "7. Scrutinize whether the report is grounded in quantifiable, concrete data or vague, unverifiable statements, and communicate your findings.\n",
    "8. Start your answer with a \"[[YES]]\"\" or \"\"[[NO]]\"\" depending on whether you would answer the question with a yes or no. Always complement your judgement on yes or no with a short explanation that summarizes the sources in an informative way, i.e. provide details.\n",
    "\n",
    "Format your answer in JSON format with the two keys: ANSWER (this should contain your answer string without sources), and SOURCES (this should be a list of the SOURCE numbers that were referenced in your answer).\n",
    "Your FINAL_ANSWER in JSON (ensure there's no format error):\n",
    "\"\"\")\n",
    "\n",
    "disclosure_prompt = PromptTemplate(\n",
    "    template=PROMPT_TEMPLATE,\n",
    "    input_variables=[\"query\", \"sources\", \"guideline\", \"answer_length\"]\n",
    ")\n",
    "\n",
    "SYSTEM_PROMPT = \"You are an AI assistant in the role of a Senior Equity Analyst with expertise in sustainability reporting that analyzes companys' annual reports.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d8d891c-c739-4bc7-971e-4703204e6ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_name = \"gpt-4.1-mini-2025-04-14\"\n",
    "openai_embedding_model = OpenAIEmbeddings() # initialize embedding model\n",
    "openai_llm = ChatOpenAI(model=llm_name, temperature=0, max_tokens=MAX_TOKEN) # initialize LLM\n",
    "\n",
    "## 5. Generate Augmented LLM answer ##\n",
    "def generate_llm_answer(report_list, section_text_dict, llm, answer_length=ANSWER_LENGTH):\n",
    "    final_results_by_report = {}\n",
    "\n",
    "    for report in report_list:\n",
    "        print(f\"--- Generating Answers for Report: {report} ---\")\n",
    "        report_assessments = {}\n",
    "        \n",
    "        for key, query_text in QUERIES.items():\n",
    "            context_str = _docs_to_string(section_text_dict[report].get(key, []), with_source=True)\n",
    "            \n",
    "            # Skip if no context is found\n",
    "            if not context_str.strip():\n",
    "                print(f\"    -> Skipping key '{key}' due to empty context.\")\n",
    "                report_assessments[key] = {\n",
    "                    \"verdict\": \"N/A\",\n",
    "                    \"analysis\": \"N/A\",\n",
    "                    \"sources\": []\n",
    "                }\n",
    "                continue\n",
    "\n",
    "            # 1. Format the prompt for the current question\n",
    "            current_prompt_text = disclosure_prompt.format(\n",
    "                query=query_text,\n",
    "                sources=context_str,\n",
    "                guideline=GUIDELINES.get(key, \"\"),\n",
    "                answer_length=answer_length\n",
    "            )\n",
    "            \n",
    "            current_message = [\n",
    "                SystemMessage(content=SYSTEM_PROMPT),\n",
    "                HumanMessage(content=current_prompt_text)\n",
    "            ]\n",
    "\n",
    "            # 2. Make a single, synchronous API call\n",
    "            response = llm.invoke(current_message)\n",
    "            text = response.content\n",
    "                \n",
    "            # 3. Parse the output            \n",
    "            try:\n",
    "                parsed_json = json.loads(text)\n",
    "                answer = parsed_json.get(\"ANSWER\", \"\")\n",
    "                sources = parsed_json.get(\"SOURCES\", [])\n",
    "\n",
    "            except (json.JSONDecodeError, TypeError) as e:\n",
    "                print(f\"[Warning] JSON parsing failed for key '{key}' in report '{report}': {e}\")\n",
    "                answer = _find_answer(text)\n",
    "                sources = _find_sources(text)\n",
    "                \n",
    "            verdict = _find_verdict(answer)\n",
    "            report_assessments[key] = {\n",
    "                \"verdict\": verdict,\n",
    "                \"analysis\": answer,\n",
    "                \"sources\": sources\n",
    "            }\n",
    "\n",
    "        final_results_by_report[report] = report_assessments\n",
    "        \n",
    "    return final_results_by_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd8074ac-8542-4b9e-a6e9-293e2dbbfe7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Processing: SchneiderElectric_2024\n",
      "\n",
      " Processing: ContinentalAG_2024\n",
      "--- Generating Answers for Report: SchneiderElectric_2024 ---\n",
      "--- Generating Answers for Report: ContinentalAG_2024 ---\n",
      "Processing time per report in minutes 6.963438194990158\n",
      "\n",
      "--- LLM Evaluation: Evaluating performance on verdicts ---\n",
      "\n",
      " Dropped 0 of 130 queries due to missing verdicts.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.808, 'precision': 0.811, 'recall': 0.808, 'f1_score': 0.796}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Your pipeline execution \n",
    "retrieved_chunks = {}  # Holds retrieved text chunks per report\n",
    "report_ids = []        # Stores report IDs for final LLM analysis\n",
    "\n",
    "# Loop through each report in your sample DataFrame\n",
    "for idx, row in sample.iterrows():\n",
    "    company_name = row['company_withAccessInfo']\n",
    "    report_id = f\"{prepare_filename(company_name)}_2024\".replace(\" \", \"\")\n",
    "    print(f\"\\n Processing: {report_id}\")\n",
    "\n",
    "    # 1. Parse the document\n",
    "    path = f\"./sample_reports/{report_id}.pdf\"\n",
    "    pages, _ = parse_pdf(path=path)\n",
    "\n",
    "    # 2. Chunk the text\n",
    "    chunks, metadata = chunk_text(pages, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "\n",
    "    # 3. Generate and store vector representations\n",
    "    db_path = f\"./faiss_db_Baseline1/{report_id}\"\n",
    "    vectorstore = get_vectorstore(chunks, metadata, db_path, embedding_model=openai_embedding_model)\n",
    "\n",
    "    # 4. Retrieve the relevant chunks\n",
    "    result = retrieve_chunks(vectorstore, queries=QUERIES, report_id=report_id, top_k=TOP_K)\n",
    "    retrieved_chunks.update(result)\n",
    "    report_ids.append(report_id)\n",
    "\n",
    "# 5. Generate LLM answers\n",
    "final_analysis = generate_llm_answer(report_ids, retrieved_chunks, openai_llm)\n",
    "\n",
    "\n",
    "time_per_report = (time.time() - start_time) / len(report_ids) / 60  # in minutes\n",
    "print(\"Processing time per report in minutes\", time_per_report)\n",
    "\n",
    "evaluate_verdicts(\n",
    "    validation_set,\n",
    "    final_analysis,\n",
    "    config_name=\"Baseline 1\",\n",
    "    llm_name=\"OpenAI: gpt-4.1-mini-2025-04-14\",\n",
    "    embedding_model=\"OpenAI: text-embedding-ada-002\",\n",
    "    resources=f\"{time_per_report:.2f} min/report\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "752d96b8-acd6-4360-a23b-cf7057a4237b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Config                        LLM                 Embedding Model  \\\n",
      "0  Baseline 1  OpenAI: gpt-4o-2024-08-06  OpenAI: text-embedding-ada-002   \n",
      "\n",
      "   Accuracy  Recall  Precision  F1-Score        Resources  \n",
      "0     0.808   0.808      0.811     0.796  6.96 min/report  \n"
     ]
    }
   ],
   "source": [
    "df_summary = pd.DataFrame(summary_results)\n",
    "print(df_summary)\n",
    "df_summary.to_excel(\"rag_comparison.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59555a58-047c-4682-bc1c-7e486acfe137",
   "metadata": {},
   "source": [
    "## 2. Llama 3.1 8B and Qwen 0.6B\n",
    "Llama 3.1 8B\n",
    "- context length: 128k https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct\n",
    "- adjustments needed: batching for efficient workflow, b/c computation on own ressources\n",
    "\n",
    "- Llama performed badly in keeping the JSON format that was prompted --> change in the System Prompt\n",
    "\n",
    "Qwen 0.6B\n",
    "- Place 4 in MTEB Leaderboard (26.06.2025), best for 2 GB Memory Usage https://huggingface.co/spaces/mteb/leaderboard\n",
    "- Number of Paramaters: 0.6B\n",
    "- Context Length: 32k\n",
    "- Embedding Dimension: Up to 1024, supports user-defined output dimensions ranging from 32 to 1024\n",
    "https://huggingface.co/Qwen/Qwen3-Embedding-0.6B "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25b23fc1-908e-4a45-94ea-6e42fa188030",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = (\"\"\"\n",
    "You are a senior sustainabiliy analyst with expertise in the european reporting standards evaluating a company's disclosure on social sustainability.\n",
    "\n",
    "\n",
    "You are presented with the following sources from the company's annual report:\n",
    "--------------------- [BEGIN OF SOURCES]\\n\n",
    "{sources}\\n\n",
    "--------------------- [END OF SOURCES]\\n\n",
    "\n",
    "Given the sources information and no prior knowledge, your main task is to respond to the posed question encapsulated in \"||\".\n",
    "Question: ||{query}||\n",
    "\n",
    "Please consider the following additional explanation to the question encapsulated in \"+++++\" as crucial for answering the question:\n",
    "+++++ [BEGIN OF EXPLANATION]\n",
    "{guideline}\n",
    "+++++ [END OF EXPLANATION]\n",
    "\n",
    "### Response Instructions ###\n",
    "Please enforce to the following guidelines in your ANSWER:\n",
    "1. Your response must be precise, thorough, and grounded on specific extracts from the report to verify its authenticity.\n",
    "2. If you are unsure, simply acknowledge the lack of knowledge, rather than fabricating an answer.\n",
    "3. Be skeptical to the information disclosed in the report as there might be greenwashing (exaggerating the firm's environmental responsibility). Always answer in a critical tone.\n",
    "4. Cheap talks are statements that are costless to make and may not necessarily reflect the true intentions or future actions of the company. Be critical for all cheap talks you discovered in the report.\n",
    "5. Always acknowledge that the information provided is representing the company's view based on its report.\n",
    "6. Scrutinize whether the report is grounded in quantifiable, concrete data or vague, unverifiable statements, and communicate your findings.\n",
    "7. Start your ANSWER with a \"[[YES]]\"\" or \"\"[[NO]]\"\" depending on whether you would answer the question with a yes or no. Always complement your judgement on yes or no with a short explanation that summarizes the sources in an informative way, i.e. provide details.\n",
    "8. Keep your ANSWER within {answer_length} words.\n",
    "\n",
    "### Formatting Instructions ###\n",
    "- Format your answer in JSON format with the two keys: ANSWER (this should contain your answer string without sources), and SOURCES (this should be a list of the SOURCE numbers that were referenced in your answer).\n",
    "- Your response **must** be returned as a **valid JSON object**.\n",
    "- Only output the JSON object — no preamble, no markdown, no extra commentary.\n",
    "- Use this exact format for your final output:\n",
    "{{\n",
    "  \"ANSWER\": \"[[YES]] or [[NO]] Here follows your explanation\",\n",
    "  \"SOURCES\": [\"1\", \"216\", \"181-182\", \"174\"]\n",
    "}}\n",
    "\n",
    "Your FINAL_ANSWER in JSON (ensure there's no format error):\n",
    "\"\"\")\n",
    "\n",
    "disclosure_prompt = PromptTemplate(\n",
    "    template=PROMPT_TEMPLATE,\n",
    "    input_variables=[\"query\", \"sources\", \"guideline\", \"answer_length\"]\n",
    ")\n",
    "\n",
    "SYSTEM_PROMPT = \"You are an AI assistant in the role of a Senior Equity Analyst with expertise in sustainability reporting that analyzes companys' annual reports.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c63ec23c-36a0-454a-a697-617184ffde50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e71c4c8ac43e4a3994f7c8f9c0d66c4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE=32\n",
    "TEMPERATURE=0.01\n",
    "\n",
    "### loading the model\n",
    "llm_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\" # important for batching since Llama is a decoder-only architecture\n",
    "\n",
    "# to reduce memory usage and speed up performance\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True, # maximizing speed and minimizing memory\n",
    "                                         bnb_4bit_compute_dtype=torch.bfloat16, # computations in bfloat16\n",
    "                                         bnb_4bit_use_double_quant=True,\n",
    "                                         bnb_4bit_quant_type= \"nf4\"\n",
    "                                         )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    llm_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "\n",
    "# enable evaluation mode to allow model inference\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded on {device}\")\n",
    "\n",
    "generate_text = transformers.pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task='text-generation',\n",
    "    return_full_text=True,   # Important for parsing logic\n",
    "    temperature=TEMPERATURE,        \n",
    "    max_new_tokens=MAX_TOKEN, \n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b2597cd-a2b5-4cdc-8cc5-90fda6f048d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "BATCH_SIZE=64\n",
    "\n",
    "generate_text = transformers.pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task='text-generation',\n",
    "    return_full_text=True,   # Important for parsing logic\n",
    "    temperature=TEMPERATURE,        \n",
    "    max_new_tokens=MAX_TOKEN, \n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dfeeca7-af94-4f97-b212-2d0ebdaae811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the new embedding model\n",
    "embeddings_qwen = HuggingFaceEmbeddings(\n",
    "    model_name=\"Qwen/Qwen3-Embedding-0.6B\",\n",
    "    model_kwargs={'device': 'cuda'} # specify device='cpu' if GPU not available \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "038d7358-a217-43f1-afd7-82e0185de088",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5. Inference with Llama ###\n",
    "\n",
    "    ## A) Prepare Prompts and Metadata ##\n",
    "def prepare_prompts(report_list, section_text_dict):\n",
    "    print(\"--- Step 1: Preparing all prompts ---\")\n",
    "    prompts_to_process = []\n",
    "    metadata_for_prompts = []\n",
    "    # Initialize a dict to hold results, including skipped items\n",
    "    final_results = {report: {} for report in report_list}\n",
    "\n",
    "    for report in report_list:\n",
    "        for key, query_text in QUERIES.items():\n",
    "            context_str = _docs_to_string(section_text_dict[report].get(key, []))\n",
    "\n",
    "            if not context_str.strip():\n",
    "                print(f\"  -> Skipping '{report}/{key}' due to empty context.\")\n",
    "                final_results[report][key] = {\n",
    "                    \"verdict\": \"NO\",\n",
    "                    \"analysis\": \"No relevant context was found to answer the question.\",\n",
    "                    \"sources\": []\n",
    "                }\n",
    "                continue\n",
    "\n",
    "            prompt_text = disclosure_prompt.format(\n",
    "                query=query_text,\n",
    "                sources=context_str,\n",
    "                guideline=GUIDELINES.get(key, \"\"),\n",
    "                answer_length=ANSWER_LENGTH\n",
    "            )\n",
    "            \n",
    "            prompts_to_process.append([\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": prompt_text}\n",
    "            ])\n",
    "            metadata_for_prompts.append({\"report\": report, \"key\": key})\n",
    "            \n",
    "    return prompts_to_process, metadata_for_prompts, final_results\n",
    "\n",
    "\n",
    "    ## B) Run batched Inference ##\n",
    "def run_batched_inference(prompts, generate_text_pipeline, batch_size=BATCH_SIZE):\n",
    "    print(f\"\\n--- Step 2: Sending {len(prompts)} prompts to the pipeline ---\")\n",
    "    if not prompts:\n",
    "        print(\"No prompts to process.\")\n",
    "        return []\n",
    "    \n",
    "    # The pipeline automatically handles tokenization and batching\n",
    "    start_time = time.time()\n",
    "    responses = generate_text_pipeline(prompts, batch_size=batch_size)\n",
    "    # empty GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"Generated LLM answers — Computation time: {(time.time() - start_time) / 60:.2f} minutes\")\n",
    "    return responses\n",
    "\n",
    "\n",
    "    ## C) Parse the results ##\n",
    "def parse_results(responses, metadata, existing_results):\n",
    "    print(\"\\n--- Step 3: Parsing all responses ---\")\n",
    "    for meta, response in zip(metadata, responses):\n",
    "        report = meta[\"report\"]\n",
    "        key = meta[\"key\"]\n",
    "        \n",
    "        full_text = response[0]['generated_text'][-1]['content']\n",
    "        \n",
    "        json_match = re.search(r'\\{.*?\\}', full_text, re.DOTALL)\n",
    "        \n",
    "        if json_match:\n",
    "            json_str = json_match.group(0)\n",
    "            try:\n",
    "                parsed_json = json.loads(json_str)\n",
    "                answer = parsed_json.get(\"ANSWER\", \"\")\n",
    "                sources = parsed_json.get(\"SOURCES\", [])\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"\\n--- JSON decode error with {report} {key} ---\\n{e}\\nProblematic JSON:\\n{json_str}\\nFull text:\\n{full_text}\\n\")\n",
    "                answer = _find_answer(full_text)\n",
    "                sources = _find_sources(full_text)\n",
    "        else:\n",
    "            print(\"No JSON found. This was the LLM response:\", full_text)\n",
    "            answer = _find_answer(full_text)\n",
    "            sources = _find_sources(full_text)\n",
    "\n",
    "        verdict = _find_verdict(answer)\n",
    "        if verdict in [None, \"N/A\"]:\n",
    "            answer = \"N/A\"\n",
    "            sources = \"N/A\"\n",
    "            print(f\"\\n--- Verdict not found in {report} {key} ---\\nFull LLM response:\\n{full_text}\\n\")\n",
    "        \n",
    "        existing_results[report][key] = {\n",
    "            \"verdict\": verdict,\n",
    "            \"analysis\": answer,\n",
    "            \"sources\": sources\n",
    "        }\n",
    "        \n",
    "    return existing_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a114b158-053c-42be-9925-25bf1f3b849b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Processing: SchneiderElectric_2024\n",
      "\n",
      " Processing: ContinentalAG_2024\n",
      "--- Step 1: Preparing all prompts ---\n",
      "\n",
      "--- Step 2: Sending 130 prompts to the pipeline ---\n",
      "Generated LLM answers — Computation time: 3.58 minutes\n",
      "\n",
      "--- Step 3: Parsing all responses ---\n",
      "Processing time per report in minutes 1.8485245803991954\n",
      "\n",
      "--- LLM Evaluation: Evaluating performance on verdicts ---\n",
      "\n",
      " Dropped 0 of 130 queries due to missing verdicts.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.769, 'precision': 0.788, 'recall': 0.769, 'f1_score': 0.774}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Your pipeline execution \n",
    "retrieved_chunks = {}  # Holds retrieved text chunks per report\n",
    "report_ids = []        # Stores report IDs for final LLM analysis\n",
    "\n",
    "# Loop through each report in your sample DataFrame\n",
    "for idx, row in sample.iterrows():\n",
    "    company_name = row['company_withAccessInfo']\n",
    "    report_id = f\"{prepare_filename(company_name)}_2024\".replace(\" \", \"\")\n",
    "    print(f\"\\n Processing: {report_id}\")\n",
    "\n",
    "    # 1. Parse the document\n",
    "    path = f\"./sample_reports/{report_id}.pdf\"\n",
    "    pages, _ = parse_pdf(path=path)\n",
    "\n",
    "    # 2. Chunk the text\n",
    "    chunks, metadata = chunk_text(pages, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "\n",
    "    # 3. Generate and store vector representations\n",
    "    db_path = f\"./faiss_db_Baseline2/{report_id}\"\n",
    "    vectorstore = get_vectorstore(chunks, metadata, db_path, embedding_model=embeddings_qwen)\n",
    "\n",
    "    # 4. Retrieve the relevant chunks\n",
    "    result = retrieve_chunks(vectorstore, queries=QUERIES, report_id=report_id, top_k=TOP_K)\n",
    "    retrieved_chunks.update(result)\n",
    "    report_ids.append(report_id)\n",
    "\n",
    "# 5. Generate LLM answers\n",
    "    # 1. Prepare prompts\n",
    "prompts, metadata, final_analysis = prepare_prompts(\n",
    "    report_list=report_ids,\n",
    "    section_text_dict=retrieved_chunks\n",
    ")\n",
    "    # 2. Run inference\n",
    "model_responses = run_batched_inference(prompts, generate_text)\n",
    "    # 3. Parse and finalize results\n",
    "final_analysis = parse_results(\n",
    "    model_responses,\n",
    "    metadata,\n",
    "    final_analysis\n",
    ")\n",
    "\n",
    "time_per_report = (time.time() - start_time) / len(report_ids) / 60  # in minutes\n",
    "print(\"Processing time per report in minutes\", time_per_report)\n",
    "\n",
    "evaluate_verdicts(\n",
    "    validation_set,\n",
    "    final_analysis,\n",
    "    config_name=\"Baseline 2\",\n",
    "    llm_name=\"Llama 3.1 8B - Instruct\",\n",
    "    embedding_model=\"Qwen 0.6B\",\n",
    "    resources=f\"{time_per_report:.2f} min/report\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "273fbe46-8b2d-4d73-bf06-1113b92d0772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Config                        LLM                 Embedding Model  \\\n",
      "0  Baseline 1  OpenAI: gpt-4o-2024-08-06  OpenAI: text-embedding-ada-002   \n",
      "1  Baseline 2    Llama 3.1 8B - Instruct                       Qwen 0.6B   \n",
      "\n",
      "   Accuracy  Recall  Precision  F1-Score        Resources  \n",
      "0     0.808   0.808      0.811     0.796  6.96 min/report  \n",
      "1     0.769   0.769      0.788     0.774  1.85 min/report  \n"
     ]
    }
   ],
   "source": [
    "df_summary = pd.DataFrame(summary_results)\n",
    "print(df_summary)\n",
    "df_summary.to_excel(\"rag_comparison.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbba1be4-1097-43bd-a30a-f6f51d318d4a",
   "metadata": {},
   "source": [
    "## 3. Llama 3.1 8B and Qwen 4B\n",
    "- Place 3 in MTEB Leaderboard (16.07.2025), 2nd best open source https://huggingface.co/spaces/mteb/leaderboard\n",
    "- Number of Paramaters: 4B\n",
    "- Context Length: 32k\n",
    "- Embedding Dimension: Up to 2560, supports user-defined output dimensions ranging from 32 to 2560\n",
    "  https://huggingface.co/Qwen/Qwen3-Embedding-4B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e2c5763-cec2-4b82-9373-7c3a57d286bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c490c98b86b4e15a732ff449b4a5b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the embedding model\n",
    "embeddings_qwen_4B = HuggingFaceEmbeddings(\n",
    "    model_name=\"Qwen/Qwen3-Embedding-4B\",\n",
    "    model_kwargs={'device': 'cuda'} # specify device='cpu' if GPU not available \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85961ced-80cb-4b9f-a53e-f63a5a68058c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Processing: SchneiderElectric_2024\n",
      "\n",
      " Processing: ContinentalAG_2024\n",
      "--- Step 1: Preparing all prompts ---\n",
      "\n",
      "--- Step 2: Sending 130 prompts to the pipeline ---\n",
      "Generated LLM answers — Computation time: 3.12 minutes\n",
      "\n",
      "--- Step 3: Parsing all responses ---\n",
      "Processing time per report in minutes 9.324741073449452\n",
      "\n",
      "--- LLM Evaluation: Evaluating performance on verdicts ---\n",
      "\n",
      " Dropped 0 of 130 queries due to missing verdicts.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7, 'precision': 0.733, 'recall': 0.7, 'f1_score': 0.708}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Your pipeline execution \n",
    "retrieved_chunks = {}  # Holds retrieved text chunks per report\n",
    "report_ids = []        # Stores report IDs for final LLM analysis\n",
    "\n",
    "# Loop through each report in your sample DataFrame\n",
    "for idx, row in sample.iterrows():\n",
    "    company_name = row['company_withAccessInfo']\n",
    "    report_id = f\"{prepare_filename(company_name)}_2024\".replace(\" \", \"\")\n",
    "    print(f\"\\n Processing: {report_id}\")\n",
    "\n",
    "    # 1. Parse the document\n",
    "    path = f\"./sample_reports/{report_id}.pdf\"\n",
    "    pages, _ = parse_pdf(path=path)\n",
    "\n",
    "    # 2. Chunk the text\n",
    "    chunks, metadata = chunk_text(pages, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "\n",
    "    # 3. Generate and store vector representations\n",
    "    db_path = f\"./faiss_db_Baseline3/{report_id}\"\n",
    "    vectorstore = get_vectorstore(chunks, metadata, db_path, embedding_model=embeddings_qwen_4B)\n",
    "\n",
    "    # 4. Retrieve the relevant chunks\n",
    "    result = retrieve_chunks(vectorstore, queries=QUERIES, report_id=report_id, top_k=TOP_K)\n",
    "    retrieved_chunks.update(result)\n",
    "    report_ids.append(report_id)\n",
    "\n",
    "# 5. Generate LLM answers\n",
    "    # 1. Prepare prompts\n",
    "prompts, metadata, final_analysis = prepare_prompts(\n",
    "    report_list=report_ids,\n",
    "    section_text_dict=retrieved_chunks\n",
    ")\n",
    "    # 2. Run inference\n",
    "model_responses = run_batched_inference(prompts, generate_text)\n",
    "    # 3. Parse and finalize results\n",
    "final_analysis = parse_results(\n",
    "    model_responses,\n",
    "    metadata,\n",
    "    final_analysis\n",
    ")\n",
    "\n",
    "time_per_report = (time.time() - start_time) / len(report_ids) / 60  # in minutes\n",
    "print(\"Processing time per report in minutes\", time_per_report)\n",
    "\n",
    "evaluate_verdicts(\n",
    "    validation_set,\n",
    "    final_analysis,\n",
    "    config_name=\"Baseline 3\",\n",
    "    llm_name=\"Llama 3.1 8B - Instruct\",\n",
    "    embedding_model=\"Qwen 4B\",\n",
    "    resources=f\"{time_per_report:.2f} min/report\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c53f64f-67b4-4b60-ab3f-bbe578aaeecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Config                        LLM                 Embedding Model  \\\n",
      "0  Baseline 1  OpenAI: gpt-4o-2024-08-06  OpenAI: text-embedding-ada-002   \n",
      "1  Baseline 2    Llama 3.1 8B - Instruct                       Qwen 0.6B   \n",
      "2  Baseline 3    Llama 3.1 8B - Instruct                         Qwen 4B   \n",
      "\n",
      "   Accuracy  Recall  Precision  F1-Score        Resources  \n",
      "0     0.808   0.808      0.811     0.796  6.96 min/report  \n",
      "1     0.769   0.769      0.788     0.774  1.85 min/report  \n",
      "2     0.700   0.700      0.733     0.708  9.32 min/report  \n"
     ]
    }
   ],
   "source": [
    "# read in old summary\n",
    "old_comparison = pd.read_excel('rag_comparison.xlsx')\n",
    "\n",
    "df_new = pd.DataFrame(summary_results)  # Your newly calculated metrics\n",
    "df_summary = pd.concat([old_comparison, df_new], ignore_index=True)\n",
    "print(df_summary)\n",
    "\n",
    "# Save back to the same file\n",
    "df_summary.to_excel(\"rag_comparison.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691e30e9-dec1-4855-a084-44d2aa8ed5e0",
   "metadata": {},
   "source": [
    "## 4. Llama 3.3 70B and Qwen 0.6B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a85156a-78b3-4233-92b3-ccf780b6699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=16\n",
    "TEMPERATURE=0.01\n",
    "\n",
    "### loading the model\n",
    "llm_name = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\" # important for batching since Llama is a decoder-only architecture\n",
    "\n",
    "# to reduce memory usage and speed up performance\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True, # maximizing speed and minimizing memory\n",
    "                                         bnb_4bit_compute_dtype=torch.bfloat16, # computations in bfloat16\n",
    "                                         bnb_4bit_use_double_quant=True,\n",
    "                                         bnb_4bit_quant_type= \"nf4\"\n",
    "                                         )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    llm_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "\n",
    "# enable evaluation mode to allow model inference\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded on {device}\")\n",
    "\n",
    "generate_text = transformers.pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task='text-generation',\n",
    "    return_full_text=True,   # Important for parsing logic\n",
    "    temperature=TEMPERATURE,        \n",
    "    max_new_tokens=MAX_TOKEN, \n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Initialize the new embedding model\n",
    "embeddings_qwen = HuggingFaceEmbeddings(\n",
    "    model_name=\"Qwen/Qwen3-Embedding-0.6B\",\n",
    "    model_kwargs={'device': 'cuda'} # specify device='cpu' if GPU not available \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1de442f5-8407-4706-b245-fba5d0cfb148",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Processing: SchneiderElectric_2024\n",
      "\n",
      " Processing: ContinentalAG_2024\n",
      "--- Step 1: Preparing all prompts ---\n",
      "\n",
      "--- Step 2: Sending 130 prompts to the pipeline ---\n",
      "Generated LLM answers — Computation time: 26.94 minutes\n",
      "\n",
      "--- Step 3: Parsing all responses ---\n",
      "No JSON found. This was the LLM response: [[NO]] The company does not explicitly confirm that all materially impacted individuals in its own workforce are considered. While it provides HR statistics covering about 88% of its employees, it excludes around 16,206 employees from non-integrated entities, indicating a lack of comprehensive inclusion.\n",
      "\"SOURCES\": [\"351\", \"245\"]\n",
      "No JSON found. This was the LLM response: [[YES]] The company discloses the frequency of engagement with its own workforce through various surveys and metrics, such as the engagement survey, which shows that 73% of employees are engaged, and 78% of employees agree on the positive impact of action plans. Additionally, the company reports on the percentage of employees covered by collective bargaining agreements and those covered by employee representation.\n",
      "\"SOURCES\": [\"211\", \"212\", \"214\"]\n",
      "No JSON found. This was the LLM response: [[YES]] The company discloses that it engaged directly with its own workforce or workers’ representatives in identifying lessons learned or improvements based on performance against workforce-related targets. This is evident from the sources, where it is stated that \"dialogue is a key enabler\" and that the company conducts surveys to identify areas of strength and improvement, and uses insights to inform action plans. Additionally, the company reports on the presence of employee representation bodies and the percentage of employees covered by collective agreements.\n",
      "\"SOURCES\": [\"208\", \"212\", \"214\", \"211\"]\n",
      "No JSON found. This was the LLM response: [[YES]] The company's policy on discrimination among its workforce specifically addresses grounds of discrimination, as it reinforces employees' rights and responsibilities, and ensures that all employees feel uniquely valued and safe to contribute their best, free from harassment, victimization, and discrimination of any kind. The company also commits to the United Nations Free and Equal Standards of Conduct for Business on Tackling Discrimination against LGBT+ People, and has a zero-tolerance policy for any kind of harassment or discrimination in the workplace.\n",
      "\"SOURCES\": [\"222\", \"226\"]\n",
      "No JSON found. This was the LLM response: [[YES]] The company describes how it ensures the effectiveness of the channels for its workforce to raise concerns or needs through various measures, including an internal survey called OneVoice to measure employee satisfaction, and a Trust Charter and Ethics & Compliance program that offers multiple channels for employees to speak up without fear of retaliation. The company also conducts a risk analysis locally for mitigation plans where relevant, and has a whistleblowing policy and grievance mechanisms in place.\n",
      "\"SOURCES\": [\"106\", \"316\", \"214\"]\n",
      "No JSON found. This was the LLM response: [[YES]] The company describes actions it has taken to provide or enable remedy for actual material impacts on its own workforce, such as introducing updated e-learning for HR internal investigators to ensure impartiality and consistent practices, and increasing the number of HR investigators to bolster investigation. Additionally, the company aims to balance its environmental commitments with the well-being and development of its workforce, and has a Committee that reviews the social impact of major re-organization projects and major human resource policies.\n",
      "\"SOURCES\": [\"223\", \"206\", \"449\"]\n",
      "No JSON found. This was the LLM response: [[YES]] The company describes how it ensures the effectiveness of the channels for its workforce to raise concerns or needs through dedicated feedback channels, HR processes, and reported metrics such as \"Percentage of employees covered by employee representation\". Additionally, the company engages with its workforce via various channels, including surveys, webcasts, and town hall formats, to gain insights into their interests and concerns.\n",
      "\"SOURCES\": [\"180\", \"182\", \"216\"]\n",
      "Processing time per report in minutes 14.767222225666046\n",
      "\n",
      "--- LLM Evaluation: Evaluating performance on verdicts ---\n",
      "\n",
      " Dropped 0 of 130 queries due to missing verdicts.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.738, 'precision': 0.731, 'recall': 0.738, 'f1_score': 0.733}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Your pipeline execution \n",
    "retrieved_chunks = {}  # Holds retrieved text chunks per report\n",
    "report_ids = []        # Stores report IDs for final LLM analysis\n",
    "\n",
    "# Loop through each report in your sample DataFrame\n",
    "for idx, row in sample.iterrows():\n",
    "    company_name = row['company_withAccessInfo']\n",
    "    report_id = f\"{prepare_filename(company_name)}_2024\".replace(\" \", \"\")\n",
    "    print(f\"\\n Processing: {report_id}\")\n",
    "\n",
    "    # 1. Parse the document\n",
    "    path = f\"./sample_reports/{report_id}.pdf\"\n",
    "    pages, _ = parse_pdf(path=path)\n",
    "\n",
    "    # 2. Chunk the text\n",
    "    chunks, metadata = chunk_text(pages, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "\n",
    "    # 3. Generate and store vector representations\n",
    "    db_path = f\"./faiss_db_Baseline4/{report_id}\"\n",
    "    vectorstore = get_vectorstore(chunks, metadata, db_path, embedding_model=embeddings_qwen)\n",
    "\n",
    "    # 4. Retrieve the relevant chunks\n",
    "    result = retrieve_chunks(vectorstore, queries=QUERIES, report_id=report_id, top_k=TOP_K)\n",
    "    retrieved_chunks.update(result)\n",
    "    report_ids.append(report_id)\n",
    "\n",
    "# 5. Generate LLM answers\n",
    "    # 1. Prepare prompts\n",
    "prompts, metadata, final_analysis = prepare_prompts(\n",
    "    report_list=report_ids,\n",
    "    section_text_dict=retrieved_chunks\n",
    ")\n",
    "    # 2. Run inference\n",
    "model_responses = run_batched_inference(prompts, generate_text)\n",
    "    # 3. Parse and finalize results\n",
    "final_analysis = parse_results(\n",
    "    model_responses,\n",
    "    metadata,\n",
    "    final_analysis\n",
    ")\n",
    "\n",
    "time_per_report = (time.time() - start_time) / len(report_ids) / 60  # in minutes\n",
    "print(\"Processing time per report in minutes\", time_per_report)\n",
    "\n",
    "evaluate_verdicts(\n",
    "    validation_set,\n",
    "    final_analysis,\n",
    "    config_name=\"Baseline 4\",\n",
    "    llm_name=\"Llama 3.3 70B - Instruct\",\n",
    "    embedding_model=\"Qwen 0.6B\",\n",
    "    resources=f\"{time_per_report:.2f} min/report\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0e57cea-b9cd-4e82-99b8-b45c4fae166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in old summary\n",
    "old_comparison = pd.read_excel('rag_comparison.xlsx')\n",
    "\n",
    "df_new = pd.DataFrame(summary_results)  # Your newly calculated metrics\n",
    "df_summary = pd.concat([old_comparison, df_new], ignore_index=True)\n",
    "\n",
    "# Save back to the same file\n",
    "df_summary.to_excel(\"rag_comparison.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0df6477-66fe-4750-abfe-b0aa16461e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Config                        LLM                 Embedding Model  \\\n",
      "0  Baseline 1  OpenAI: gpt-4o-2024-08-06  OpenAI: text-embedding-ada-002   \n",
      "1  Baseline 2    Llama 3.1 8B - Instruct                       Qwen 0.6B   \n",
      "2  Baseline 3    Llama 3.1 8B - Instruct                         Qwen 4B   \n",
      "3  Baseline 4   Llama 3.3 70B - Instruct                       Qwen 0.6B   \n",
      "\n",
      "   Accuracy  Recall  Precision  F1-Score         Resources  \n",
      "0     0.808   0.808      0.811     0.796   6.96 min/report  \n",
      "1     0.769   0.769      0.788     0.774   1.85 min/report  \n",
      "2     0.700   0.700      0.733     0.708   9.32 min/report  \n",
      "3     0.738   0.738      0.731     0.733  14.77 min/report  \n"
     ]
    }
   ],
   "source": [
    "print(df_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752e01d3-1877-490a-802d-0a60b32ba79c",
   "metadata": {},
   "source": [
    "## 5. Llama 3.3 70B and Qwen 4B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6df970a3-7e2b-4d40-bef4-090c5d1040ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86cf870a0a244eba8110c37a2949cb0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6129b30a26147299483762011e2bf42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BATCH_SIZE=16\n",
    "TEMPERATURE=0.01\n",
    "\n",
    "### loading the model\n",
    "llm_name = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.padding_side = \"left\" # important for batching since Llama is a decoder-only architecture\n",
    "\n",
    "# to reduce memory usage and speed up performance\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True, # maximizing speed and minimizing memory\n",
    "                                         bnb_4bit_compute_dtype=torch.bfloat16, # computations in bfloat16\n",
    "                                         bnb_4bit_use_double_quant=True,\n",
    "                                         bnb_4bit_quant_type= \"nf4\"\n",
    "                                         )\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    llm_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "\n",
    "# enable evaluation mode to allow model inference\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded on {device}\")\n",
    "\n",
    "generate_text = transformers.pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task='text-generation',\n",
    "    return_full_text=True,   # Important for parsing logic\n",
    "    temperature=TEMPERATURE,        \n",
    "    max_new_tokens=MAX_TOKEN, \n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Initialize the new embedding model\n",
    "embeddings_qwen_4B = HuggingFaceEmbeddings(\n",
    "    model_name=\"Qwen/Qwen3-Embedding-4B\",\n",
    "    model_kwargs={'device': 'cuda'} # specify device='cpu' if GPU not available \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c57deb23-4ee7-48c0-88bc-e54defba8621",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Processing: SchneiderElectric_2024\n",
      "\n",
      " Processing: ContinentalAG_2024\n",
      "--- Step 1: Preparing all prompts ---\n",
      "\n",
      "--- Step 2: Sending 130 prompts to the pipeline ---\n",
      "Generated LLM answers — Computation time: 27.11 minutes\n",
      "\n",
      "--- Step 3: Parsing all responses ---\n",
      "No JSON found. This was the LLM response: [[YES]] The company describes activities resulting in material positive impacts on its own workforce, such as fostering an inclusive and caring environment, and implementing programs on employment, education, training, entrepreneurship, women empowerment, and non-discrimination. These initiatives aim to have a positive impact on its workforce.\n",
      "\"SOURCES\": [\"206\", \"252\"]\n",
      "No JSON found. This was the LLM response: [[YES]] The company describes actions planned or underway to pursue material opportunities related to its own workforce, such as increasing local impact through employee engagement, providing equal opportunities, and fostering an environment of diversity, equity, and inclusion. Additionally, the company mentions initiatives like hiring opportunities for interns, apprentices, and fresh graduates, as well as digital upskilling programs for employees.\n",
      "\"SOURCES\": [\"79\", \"119\", \"44\", \"203\"]\n",
      "No JSON found. This was the LLM response: [[NO]] The company's report lacks a comprehensive disclosure on ensuring its internal practices do not cause or contribute to material negative impacts on its own workforce. Although it mentions policies and processes related to working conditions, health and safety, and forced labor, there are notable gaps, such as missing structured controls to prevent slavery/trafficked labor and child labor in operations, and no policy on notice period or weekly off. These gaps suggest that the company's disclosure is incomplete.\n",
      "\"SOURCES\": [\"205\", \"245\", \"273\"]\n",
      "No JSON found. This was the LLM response: [[NO]] The company does not disclose specific operations at significant risk of incidents of forced labour or compulsory labour by countries or geographic areas. Although it mentions risks related to human rights, forced labor, and modern slavery, it does not provide concrete information on the locations or operations that are at risk. The company's statements appear to be general and focused on its commitment to refraining from working with business partners that use forced or compulsory labor, rather than providing specific details on risk areas.\n",
      "\"SOURCES\": [\"239\", \"245\", \"112\", \"122\"]\n",
      "No JSON found. This was the LLM response: [[YES]] The company discloses operations that are at significant risk of incidents of child labour by countries or geographic areas, as indicated by their participation in the \"Lab 8.7\" action-research project and the Copper Mark initiative, which aim to prevent the risks of child and forced labor in supply chains. Additionally, the company mentions incidents of discrimination, non-respect of UNGPs on Business and Human Rights, and OECD Guidelines, which may imply a significant risk of child labor or forced labor in the value chain.\n",
      "\"SOURCES\": [\"88\", \"241\", \"277\"]\n",
      "No JSON found. This was the LLM response: [[YES]] The company discloses its general approach to engage with people in its own workforce, including dialogue with employees and their representatives, as stated in its Global Human Rights Policy. This two-way dialogue is a key enabler to employees' engagement and the Company's performance. The company also conducts an engagement survey, which includes questions on employees' well-being and flexibility to modify their work arrangements.\n",
      "\"SOURCES\": [\"208\", \"211\"]\n",
      "No JSON found. This was the LLM response: [[YES]] The company discloses the type of engagement it conducts with its own workforce, including employee engagement surveys, social dialogue, and participation of employee representatives. According to the report, 74% of employees feel that the organization actively looks after their well-being and 81% have flexibility to modify their work arrangements. The company also mentions engagement with employee representatives from European countries through an agreement signed in 2014.\n",
      "\"SOURCES\": [\"208\", \"211\", \"212\"]\n",
      "No JSON found. This was the LLM response: [[YES]] The company discloses the frequency of engagement with its own workforce through various means, including an annual employee engagement survey with a high response rate of 88%, and regular dialogue with local work councils on compensation matters. The company also recognizes the importance of dialogue and engages with employee representatives from European countries as described in the agreement signed in 2014.\n",
      "\"SOURCES\": [\"79\", \"81\", \"208\", \"211\", \"212\", \"479\"]\n",
      "No JSON found. This was the LLM response: [[YES]] The company discloses that it engages directly with its own workforce or workers’ representatives in tracking performance against workforce-related targets, as stated in its Global Human Rights Policy, which enables two-way dialogue for employees' engagement and the Company's performance. This is further supported by the company's ambition to achieve a 75% engagement score by the end of 2025 and the implementation of action plans, with 78% of employees agreeing on the positive impact of action plans.\n",
      "\"SOURCES\": [\"208\", \"211\"]\n",
      "No JSON found. This was the LLM response: [[YES]] The company describes its approach and processes for providing or contributing to remedy for material negative impacts it has caused or contributed to on its own workforce. According to the report, the company has a policy stating that in situations where it has caused or contributed to a negative impact, it commits to provide or help provide remedy to those harmed. The Trust Line, an internal and external alert system, can be used by affected parties to raise concerns.\n",
      "\"SOURCES\": [\"249\", \"210\"]\n",
      "No JSON found. This was the LLM response: [[YES]] The company describes the processes through which it supports the availability of channels in the workplace of its own workforce to raise concerns or needs, including a Trust Line grievance mechanism, a whistleblowing policy, and employee engagement programs. These channels allow employees to report unethical behavior, raise concerns, and provide feedback to management.\n",
      "\"SOURCES\": [\"105\", \"110\", \"210\", \"214\", \"316\"]\n",
      "No JSON found. This was the LLM response: [[YES]] The company describes how it ensures the effectiveness of the channels for its workforce to raise concerns or needs through various measures, including an annual survey where 83% of employees reported feeling confident to report unethical behavior. The company also has a Trust Charter and Ethics & Compliance program, which offers multiple channels for employees to speak up without fear of retaliation. Additionally, the company has a grievance mechanism and a Committee that levies sanctions and remediation actions on serious non-compliance cases.\n",
      "\"SOURCES\": [\"105\", \"316\", \"41\"]\n",
      "No JSON found. This was the LLM response: [[NO]] The company does not disclose information about reconciliation of fines, penalties, and compensation for damages as a result of work-related discrimination and harassment in its workforce. Although the report mentions fines, penalties, and compensation related to human rights violations, it only provides data on the total amount of fines, penalties, and compensation paid, which is €0 million, without further details on reconciliation.\n",
      "\"SOURCES\": [\"191\", \"197\", \"201\"]\n",
      "Processing time per report in minutes 21.446335021654765\n",
      "\n",
      "--- LLM Evaluation: Evaluating performance on verdicts ---\n",
      "\n",
      " Dropped 0 of 130 queries due to missing verdicts.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.715, 'precision': 0.706, 'recall': 0.715, 'f1_score': 0.709}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Your pipeline execution \n",
    "retrieved_chunks = {}  # Holds retrieved text chunks per report\n",
    "report_ids = []        # Stores report IDs for final LLM analysis\n",
    "\n",
    "# Loop through each report in your sample DataFrame\n",
    "for idx, row in sample.iterrows():\n",
    "    company_name = row['company_withAccessInfo']\n",
    "    report_id = f\"{prepare_filename(company_name)}_2024\".replace(\" \", \"\")\n",
    "    print(f\"\\n Processing: {report_id}\")\n",
    "\n",
    "    # 1. Parse the document\n",
    "    path = f\"./sample_reports/{report_id}.pdf\"\n",
    "    pages, _ = parse_pdf(path=path)\n",
    "\n",
    "    # 2. Chunk the text\n",
    "    chunks, metadata = chunk_text(pages, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "\n",
    "    # 3. Generate and store vector representations\n",
    "    db_path = f\"./faiss_db_Baseline5/{report_id}\"\n",
    "    vectorstore = get_vectorstore(chunks, metadata, db_path, embedding_model=embeddings_qwen_4B)\n",
    "\n",
    "    # 4. Retrieve the relevant chunks\n",
    "    result = retrieve_chunks(vectorstore, queries=QUERIES, report_id=report_id, top_k=TOP_K)\n",
    "    retrieved_chunks.update(result)\n",
    "    report_ids.append(report_id)\n",
    "\n",
    "# 5. Generate LLM answers\n",
    "    # 1. Prepare prompts\n",
    "prompts, metadata, final_analysis = prepare_prompts(\n",
    "    report_list=report_ids,\n",
    "    section_text_dict=retrieved_chunks\n",
    ")\n",
    "    # 2. Run inference\n",
    "model_responses = run_batched_inference(prompts, generate_text)\n",
    "    # 3. Parse and finalize results\n",
    "final_analysis = parse_results(\n",
    "    model_responses,\n",
    "    metadata,\n",
    "    final_analysis\n",
    ")\n",
    "\n",
    "time_per_report = (time.time() - start_time) / len(report_ids) / 60  # in minutes\n",
    "print(\"Processing time per report in minutes\", time_per_report)\n",
    "\n",
    "evaluate_verdicts(\n",
    "    validation_set,\n",
    "    final_analysis,\n",
    "    config_name=\"Baseline 5\",\n",
    "    llm_name=\"Llama 3.3 70B - Instruct\",\n",
    "    embedding_model=\"Qwen 4B\",\n",
    "    resources=f\"{time_per_report:.2f} min/report\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "291f5c53-d064-4868-9d52-1e52c2b67db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Config                        LLM                 Embedding Model  \\\n",
      "0  Baseline 1  OpenAI: gpt-4o-2024-08-06  OpenAI: text-embedding-ada-002   \n",
      "1  Baseline 2    Llama 3.1 8B - Instruct                       Qwen 0.6B   \n",
      "2  Baseline 3    Llama 3.1 8B - Instruct                         Qwen 4B   \n",
      "3  Baseline 4   Llama 3.3 70B - Instruct                       Qwen 0.6B   \n",
      "4  Baseline 5   Llama 3.3 70B - Instruct                         Qwen 4B   \n",
      "\n",
      "   Accuracy  Recall  Precision  F1-Score         Resources  \n",
      "0     0.808   0.808      0.811     0.796   6.96 min/report  \n",
      "1     0.769   0.769      0.788     0.774   1.85 min/report  \n",
      "2     0.700   0.700      0.733     0.708   9.32 min/report  \n",
      "3     0.738   0.738      0.731     0.733  14.77 min/report  \n",
      "4     0.715   0.715      0.706     0.709  21.45 min/report  \n"
     ]
    }
   ],
   "source": [
    "# read in old summary\n",
    "old_comparison = pd.read_excel('rag_comparison.xlsx')\n",
    "\n",
    "df_new = pd.DataFrame(summary_results)  # Your newly calculated metrics\n",
    "df_summary = pd.concat([old_comparison, df_new], ignore_index=True)\n",
    "\n",
    "# Save back to the same file\n",
    "df_summary.to_excel(\"rag_comparison.xlsx\", index=False)\n",
    "print(df_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa02d24-2f60-473b-ad12-fa49a25362d9",
   "metadata": {},
   "source": [
    "# PDF Extraction\n",
    "## 1. PyMuPDF4LLM\n",
    "https://pymupdf.readthedocs.io/en/latest/pymupdf4llm/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fe28be0-5d62-4ab9-bf0d-cd39c71fd857",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adjusted for PyMuPDF4LLM ###\n",
    "\n",
    "## 1. Parse the Document ##\n",
    "def parse_pdf_4llm(path=None, url=None):\n",
    "    assert (path is not None) != (url is not None), \"Provide either a local path or a URL.\"\n",
    "\n",
    "    if path:\n",
    "        doc_pages = pymupdf4llm.to_markdown(path, page_chunks=True)\n",
    "    else:\n",
    "        response = requests.get(url)\n",
    "        doc_pages = pymupdf4llm.to_markdown(io.BytesIO(response.content), page_chunks=True)\n",
    "\n",
    "    pages = [page_dict[\"text\"] for page_dict in doc_pages]\n",
    "    metadata = [{\"page\": str(page_dict[\"metadata\"][\"page\"])} for page_dict in doc_pages]\n",
    "\n",
    "    full_text = \"\".join(pages)\n",
    "    return pages, full_text, metadata\n",
    "\n",
    "\n",
    "## 2. Chunk the text ##\n",
    "def chunk_text_4llm(pages, metadata, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \"],\n",
    "    )\n",
    "\n",
    "    chunks = []\n",
    "    chunk_metadata = []\n",
    "\n",
    "    for idx, page in enumerate(pages):\n",
    "        page_chunks = splitter.split_text(page)\n",
    "        chunks.extend(page_chunks)\n",
    "        chunk_metadata.extend([metadata[idx]] * len(page_chunks))\n",
    "\n",
    "    return chunks, chunk_metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f629903a-a699-4108-bfbe-7030f926510e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Processing: SchneiderElectric_2024\n",
      "\n",
      " Processing: ContinentalAG_2024\n",
      "--- Step 1: Preparing all prompts ---\n",
      "\n",
      "--- Step 2: Sending 130 prompts to the pipeline ---\n",
      "Generated LLM answers — Computation time: 2.65 minutes\n",
      "\n",
      "--- Step 3: Parsing all responses ---\n",
      "Processing time per report in minutes 5.313419810930887\n",
      "\n",
      "--- LLM Evaluation: Evaluating performance on verdicts ---\n",
      "\n",
      " Dropped 0 of 130 queries due to missing verdicts.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.654, 'precision': 0.744, 'recall': 0.654, 'f1_score': 0.661}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Your pipeline execution \n",
    "retrieved_chunks = {}  # Holds retrieved text chunks per report\n",
    "report_ids = []        # Stores report IDs for final LLM analysis\n",
    "\n",
    "# Loop through each report in your sample DataFrame\n",
    "for idx, row in sample.iterrows():\n",
    "    company_name = row['company_withAccessInfo']\n",
    "    report_id = f\"{prepare_filename(company_name)}_2024\".replace(\" \", \"\")\n",
    "    print(f\"\\n Processing: {report_id}\")\n",
    "\n",
    "    # 1. Parse the document\n",
    "    path = f\"./sample_reports/{report_id}.pdf\"\n",
    "    pages, _, metadata = parse_pdf_4llm(path=path)\n",
    "\n",
    "    # 2. Chunk the text\n",
    "    chunks, metadata = chunk_text_4llm(pages, metadata, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "\n",
    "    # 3. Generate and store vector representations\n",
    "    db_path = f\"./faiss_db_PdfExtraction1/{report_id}\"\n",
    "    vectorstore = get_vectorstore(chunks, metadata, db_path, embedding_model=embeddings_qwen)\n",
    "\n",
    "    # 4. Retrieve the relevant chunks\n",
    "    result = retrieve_chunks(vectorstore, queries=QUERIES, report_id=report_id, top_k=TOP_K)\n",
    "    retrieved_chunks.update(result)\n",
    "    report_ids.append(report_id)\n",
    "\n",
    "# 5. Generate LLM answers\n",
    "    # 1. Prepare prompts\n",
    "prompts, metadata, final_analysis = prepare_prompts(\n",
    "    report_list=report_ids,\n",
    "    section_text_dict=retrieved_chunks\n",
    ")\n",
    "    # 2. Run inference\n",
    "model_responses = run_batched_inference(prompts, generate_text)\n",
    "    # 3. Parse and finalize results\n",
    "final_analysis = parse_results(\n",
    "    model_responses,\n",
    "    metadata,\n",
    "    final_analysis\n",
    ")\n",
    "\n",
    "time_per_report = (time.time() - start_time) / len(report_ids) / 60  # in minutes\n",
    "print(\"Processing time per report in minutes\", time_per_report)\n",
    "\n",
    "evaluate_verdicts(\n",
    "    validation_set,\n",
    "    final_analysis,\n",
    "    config_name=\"PDF Extraction: MyPuPDF4LLM\",\n",
    "    llm_name=\"Llama 3.1 8B - Instruct\",\n",
    "    embedding_model=\"Qwen 0.6B\",\n",
    "    resources=f\"{time_per_report:.2f} min/report\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e134ff0-b9d8-44bb-909f-79751cdb37a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Config                        LLM  \\\n",
      "0                   Baseline 1  OpenAI: gpt-4o-2024-08-06   \n",
      "1                   Baseline 2    Llama 3.1 8B - Instruct   \n",
      "2                   Baseline 3    Llama 3.1 8B - Instruct   \n",
      "3                   Baseline 4   Llama 3.3 70B - Instruct   \n",
      "4                   Baseline 5   Llama 3.3 70B - Instruct   \n",
      "5  PDF Extraction: MyPuPDF4LLM    Llama 3.1 8B - Instruct   \n",
      "\n",
      "                  Embedding Model  Accuracy  Recall  Precision  F1-Score  \\\n",
      "0  OpenAI: text-embedding-ada-002     0.808   0.808      0.811     0.796   \n",
      "1                       Qwen 0.6B     0.769   0.769      0.788     0.774   \n",
      "2                         Qwen 4B     0.700   0.700      0.733     0.708   \n",
      "3                       Qwen 0.6B     0.738   0.738      0.731     0.733   \n",
      "4                         Qwen 4B     0.715   0.715      0.706     0.709   \n",
      "5                       Qwen 0.6B     0.654   0.654      0.744     0.661   \n",
      "\n",
      "          Resources  \n",
      "0   6.96 min/report  \n",
      "1   1.85 min/report  \n",
      "2   9.32 min/report  \n",
      "3  14.77 min/report  \n",
      "4  21.45 min/report  \n",
      "5   5.31 min/report  \n"
     ]
    }
   ],
   "source": [
    "# read in old summary\n",
    "old_comparison = pd.read_excel('rag_comparison.xlsx')\n",
    "\n",
    "df_new = pd.DataFrame(summary_results)  # Your newly calculated metrics\n",
    "df_summary = pd.concat([old_comparison, df_new], ignore_index=True)\n",
    "\n",
    "# Save back to the same file\n",
    "df_summary.to_excel(\"rag_comparison.xlsx\", index=False)\n",
    "print(df_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871ff929-2181-44e9-a9d1-0fb630783bc8",
   "metadata": {},
   "source": [
    "# Retrieval Settings\n",
    "## 1. based on Gehricke et al. (2025):\n",
    "- Chunk size 400\n",
    "- chunk overlap 50\n",
    "- top K 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7bee3ef6-d4eb-4ec1-a9a0-35d4a4c89ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Processing: SchneiderElectric_2024\n",
      "\n",
      " Processing: ContinentalAG_2024\n",
      "--- Step 1: Preparing all prompts ---\n",
      "\n",
      "--- Step 2: Sending 130 prompts to the pipeline ---\n",
      "Generated LLM answers — Computation time: 4.32 minutes\n",
      "\n",
      "--- Step 3: Parsing all responses ---\n",
      "Processing time per report in minutes 3.4678354501724242\n",
      "\n",
      "--- LLM Evaluation: Evaluating performance on verdicts ---\n",
      "\n",
      " Dropped 0 of 130 queries due to missing verdicts.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.754, 'precision': 0.764, 'recall': 0.754, 'f1_score': 0.757}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOP_K = 10\n",
    "CHUNK_SIZE = 400\n",
    "CHUNK_OVERLAP = 50\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Your pipeline execution \n",
    "retrieved_chunks = {}  # Holds retrieved text chunks per report\n",
    "report_ids = []        # Stores report IDs for final LLM analysis\n",
    "\n",
    "# Loop through each report in your sample DataFrame\n",
    "for idx, row in sample.iterrows():\n",
    "    company_name = row['company_withAccessInfo']\n",
    "    report_id = f\"{prepare_filename(company_name)}_2024\".replace(\" \", \"\")\n",
    "    print(f\"\\n Processing: {report_id}\")\n",
    "\n",
    "    # 1. Parse the document\n",
    "    path = f\"./sample_reports/{report_id}.pdf\"\n",
    "    pages, _ = parse_pdf(path=path)\n",
    "\n",
    "    # 2. Chunk the text\n",
    "    chunks, metadata = chunk_text(pages, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "\n",
    "    # 3. Generate and store vector representations\n",
    "    db_path = f\"./faiss_db_RetrievalSettings1/{report_id}\"\n",
    "    vectorstore = get_vectorstore(chunks, metadata, db_path, embedding_model=embeddings_qwen)\n",
    "\n",
    "    # 4. Retrieve the relevant chunks\n",
    "    result = retrieve_chunks(vectorstore, queries=QUERIES, report_id=report_id, top_k=TOP_K)\n",
    "    retrieved_chunks.update(result)\n",
    "    report_ids.append(report_id)\n",
    "\n",
    "# 5. Generate LLM answers\n",
    "    # 1. Prepare prompts\n",
    "prompts, metadata, final_analysis = prepare_prompts(\n",
    "    report_list=report_ids,\n",
    "    section_text_dict=retrieved_chunks\n",
    ")\n",
    "    # 2. Run inference\n",
    "model_responses = run_batched_inference(prompts, generate_text)\n",
    "    # 3. Parse and finalize results\n",
    "final_analysis = parse_results(\n",
    "    model_responses,\n",
    "    metadata,\n",
    "    final_analysis\n",
    ")\n",
    "\n",
    "time_per_report = (time.time() - start_time) / len(report_ids) / 60  # in minutes\n",
    "print(\"Processing time per report in minutes\", time_per_report)\n",
    "\n",
    "evaluate_verdicts(\n",
    "    validation_set,\n",
    "    final_analysis,\n",
    "    config_name=\"Retrieval Settings 1\",\n",
    "    llm_name=\"Llama 3.1 8B - Instruct\",\n",
    "    embedding_model=\"Qwen 0.6B\",\n",
    "    resources=f\"{time_per_report:.2f} min/report\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c85b6d8-edb5-47d3-9e8a-85f45514fd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Config                        LLM  \\\n",
      "0                   Baseline 1  OpenAI: gpt-4o-2024-08-06   \n",
      "1                   Baseline 2    Llama 3.1 8B - Instruct   \n",
      "2                   Baseline 3    Llama 3.1 8B - Instruct   \n",
      "3                   Baseline 4   Llama 3.3 70B - Instruct   \n",
      "4                   Baseline 5   Llama 3.3 70B - Instruct   \n",
      "5  PDF Extraction: MyPuPDF4LLM    Llama 3.1 8B - Instruct   \n",
      "6  PDF Extraction: MyPuPDF4LLM    Llama 3.1 8B - Instruct   \n",
      "7         Retrieval Settings 1    Llama 3.1 8B - Instruct   \n",
      "\n",
      "                  Embedding Model  Accuracy  Recall  Precision  F1-Score  \\\n",
      "0  OpenAI: text-embedding-ada-002     0.808   0.808      0.811     0.796   \n",
      "1                       Qwen 0.6B     0.769   0.769      0.788     0.774   \n",
      "2                         Qwen 4B     0.700   0.700      0.733     0.708   \n",
      "3                       Qwen 0.6B     0.738   0.738      0.731     0.733   \n",
      "4                         Qwen 4B     0.715   0.715      0.706     0.709   \n",
      "5                       Qwen 0.6B     0.654   0.654      0.744     0.661   \n",
      "6                       Qwen 0.6B     0.654   0.654      0.744     0.661   \n",
      "7                       Qwen 0.6B     0.754   0.754      0.764     0.757   \n",
      "\n",
      "          Resources  TOP_K  CHUNK_SIZE  CHUNK_OVERLAP  \n",
      "0   6.96 min/report    NaN         NaN            NaN  \n",
      "1   1.85 min/report    NaN         NaN            NaN  \n",
      "2   9.32 min/report    NaN         NaN            NaN  \n",
      "3  14.77 min/report    NaN         NaN            NaN  \n",
      "4  21.45 min/report    NaN         NaN            NaN  \n",
      "5   5.31 min/report    NaN         NaN            NaN  \n",
      "6   5.31 min/report   10.0       400.0           50.0  \n",
      "7   3.47 min/report   10.0       400.0           50.0  \n"
     ]
    }
   ],
   "source": [
    "# read in old summary\n",
    "old_comparison = pd.read_excel('rag_comparison.xlsx')\n",
    "\n",
    "df_new = pd.DataFrame(summary_results)  # Your newly calculated metrics\n",
    "summary_results = []\n",
    "df_new[\"TOP_K\"] = TOP_K\n",
    "df_new[\"CHUNK_SIZE\"] = CHUNK_SIZE\n",
    "df_new[\"CHUNK_OVERLAP\"] = CHUNK_OVERLAP\n",
    "df_summary = pd.concat([old_comparison, df_new], ignore_index=True)\n",
    "\n",
    "# Save back to the same file\n",
    "df_summary.to_excel(\"rag_comparison.xlsx\", index=False)\n",
    "print(df_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047f6ec4-1a4a-4a45-b69b-592d146b4358",
   "metadata": {},
   "source": [
    "## 2. based on Forster et al. (2025):\n",
    "- Chunk size 400\n",
    "- chunk overlap 100\n",
    "- top K 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fd9db52b-e243-4ef2-8c22-10a990e4d6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Processing: SchneiderElectric_2024\n",
      "\n",
      " Processing: ContinentalAG_2024\n",
      "--- Step 1: Preparing all prompts ---\n",
      "\n",
      "--- Step 2: Sending 130 prompts to the pipeline ---\n",
      "Generated LLM answers — Computation time: 3.89 minutes\n",
      "\n",
      "--- Step 3: Parsing all responses ---\n",
      "Processing time per report in minutes 3.4469019850095113\n",
      "\n",
      "--- LLM Evaluation: Evaluating performance on verdicts ---\n",
      "\n",
      " Dropped 0 of 130 queries due to missing verdicts.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.731, 'precision': 0.739, 'recall': 0.731, 'f1_score': 0.734}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOP_K = 10\n",
    "CHUNK_SIZE = 400\n",
    "CHUNK_OVERLAP = 100\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Your pipeline execution \n",
    "retrieved_chunks = {}  # Holds retrieved text chunks per report\n",
    "report_ids = []        # Stores report IDs for final LLM analysis\n",
    "\n",
    "# Loop through each report in your sample DataFrame\n",
    "for idx, row in sample.iterrows():\n",
    "    company_name = row['company_withAccessInfo']\n",
    "    report_id = f\"{prepare_filename(company_name)}_2024\".replace(\" \", \"\")\n",
    "    print(f\"\\n Processing: {report_id}\")\n",
    "\n",
    "    # 1. Parse the document\n",
    "    path = f\"./sample_reports/{report_id}.pdf\"\n",
    "    pages, _ = parse_pdf(path=path)\n",
    "\n",
    "    # 2. Chunk the text\n",
    "    chunks, metadata = chunk_text(pages, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "\n",
    "    # 3. Generate and store vector representations\n",
    "    db_path = f\"./faiss_db_RetrievalSettings2/{report_id}\"\n",
    "    vectorstore = get_vectorstore(chunks, metadata, db_path, embedding_model=embeddings_qwen)\n",
    "\n",
    "    # 4. Retrieve the relevant chunks\n",
    "    result = retrieve_chunks(vectorstore, queries=QUERIES, report_id=report_id, top_k=TOP_K)\n",
    "    retrieved_chunks.update(result)\n",
    "    report_ids.append(report_id)\n",
    "\n",
    "# 5. Generate LLM answers\n",
    "    # 1. Prepare prompts\n",
    "prompts, metadata, final_analysis = prepare_prompts(\n",
    "    report_list=report_ids,\n",
    "    section_text_dict=retrieved_chunks\n",
    ")\n",
    "    # 2. Run inference\n",
    "model_responses = run_batched_inference(prompts, generate_text)\n",
    "    # 3. Parse and finalize results\n",
    "final_analysis = parse_results(\n",
    "    model_responses,\n",
    "    metadata,\n",
    "    final_analysis\n",
    ")\n",
    "\n",
    "time_per_report = (time.time() - start_time) / len(report_ids) / 60  # in minutes\n",
    "print(\"Processing time per report in minutes\", time_per_report)\n",
    "\n",
    "evaluate_verdicts(\n",
    "    validation_set,\n",
    "    final_analysis,\n",
    "    config_name=\"Retrieval Settings 2\",\n",
    "    llm_name=\"Llama 3.1 8B - Instruct\",\n",
    "    embedding_model=\"Qwen 0.6B\",\n",
    "    resources=f\"{time_per_report:.2f} min/report\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1b92118-48f7-494f-abe5-2019d6eb730a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Config                        LLM  \\\n",
      "0                   Baseline 1  OpenAI: gpt-4o-2024-08-06   \n",
      "1                   Baseline 2    Llama 3.1 8B - Instruct   \n",
      "2                   Baseline 3    Llama 3.1 8B - Instruct   \n",
      "3                   Baseline 4   Llama 3.3 70B - Instruct   \n",
      "4                   Baseline 5   Llama 3.3 70B - Instruct   \n",
      "5  PDF Extraction: MyPuPDF4LLM    Llama 3.1 8B - Instruct   \n",
      "6  PDF Extraction: MyPuPDF4LLM    Llama 3.1 8B - Instruct   \n",
      "7         Retrieval Settings 1    Llama 3.1 8B - Instruct   \n",
      "8         Retrieval Settings 2    Llama 3.1 8B - Instruct   \n",
      "\n",
      "                  Embedding Model  Accuracy  Recall  Precision  F1-Score  \\\n",
      "0  OpenAI: text-embedding-ada-002     0.808   0.808      0.811     0.796   \n",
      "1                       Qwen 0.6B     0.769   0.769      0.788     0.774   \n",
      "2                         Qwen 4B     0.700   0.700      0.733     0.708   \n",
      "3                       Qwen 0.6B     0.738   0.738      0.731     0.733   \n",
      "4                         Qwen 4B     0.715   0.715      0.706     0.709   \n",
      "5                       Qwen 0.6B     0.654   0.654      0.744     0.661   \n",
      "6                       Qwen 0.6B     0.654   0.654      0.744     0.661   \n",
      "7                       Qwen 0.6B     0.754   0.754      0.764     0.757   \n",
      "8                       Qwen 0.6B     0.731   0.731      0.739     0.734   \n",
      "\n",
      "          Resources  TOP_K  CHUNK_SIZE  CHUNK_OVERLAP  \n",
      "0   6.96 min/report    NaN         NaN            NaN  \n",
      "1   1.85 min/report    NaN         NaN            NaN  \n",
      "2   9.32 min/report    NaN         NaN            NaN  \n",
      "3  14.77 min/report    NaN         NaN            NaN  \n",
      "4  21.45 min/report    NaN         NaN            NaN  \n",
      "5   5.31 min/report    NaN         NaN            NaN  \n",
      "6   5.31 min/report   10.0       400.0           50.0  \n",
      "7   3.47 min/report   10.0       400.0           50.0  \n",
      "8   3.45 min/report   10.0       400.0          100.0  \n"
     ]
    }
   ],
   "source": [
    "# read in old summary\n",
    "old_comparison = pd.read_excel('rag_comparison.xlsx')\n",
    "\n",
    "df_new = pd.DataFrame(summary_results)  # Your newly calculated metrics\n",
    "summary_results = []\n",
    "df_new[\"TOP_K\"] = TOP_K\n",
    "df_new[\"CHUNK_SIZE\"] = CHUNK_SIZE\n",
    "df_new[\"CHUNK_OVERLAP\"] = CHUNK_OVERLAP\n",
    "df_summary = pd.concat([old_comparison, df_new], ignore_index=True)\n",
    "\n",
    "# Save back to the same file\n",
    "df_summary.to_excel(\"rag_comparison.xlsx\", index=False)\n",
    "print(df_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d927e4b-0bf8-406d-b85b-5e87093443a3",
   "metadata": {},
   "source": [
    "## 3. based on Ni et al. (2023):\n",
    "- Chunk size 500\n",
    "- chunk overlap 20\n",
    "- top K 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a4ab0ff-712d-49db-af0a-fe20aa734771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Processing: SchneiderElectric_2024\n",
      "\n",
      " Processing: ContinentalAG_2024\n",
      "--- Step 1: Preparing all prompts ---\n",
      "\n",
      "--- Step 2: Sending 130 prompts to the pipeline ---\n",
      "Generated LLM answers — Computation time: 6.42 minutes\n",
      "\n",
      "--- Step 3: Parsing all responses ---\n",
      "Processing time per report in minutes 3.270663567384084\n",
      "\n",
      "--- LLM Evaluation: Evaluating performance on verdicts ---\n",
      "\n",
      " Dropped 0 of 130 queries due to missing verdicts.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.738, 'precision': 0.729, 'recall': 0.738, 'f1_score': 0.729}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOP_K = 20\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 20\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Your pipeline execution \n",
    "retrieved_chunks = {}  # Holds retrieved text chunks per report\n",
    "report_ids = []        # Stores report IDs for final LLM analysis\n",
    "\n",
    "# Loop through each report in your sample DataFrame\n",
    "for idx, row in sample.iterrows():\n",
    "    company_name = row['company_withAccessInfo']\n",
    "    report_id = f\"{prepare_filename(company_name)}_2024\".replace(\" \", \"\")\n",
    "    print(f\"\\n Processing: {report_id}\")\n",
    "\n",
    "    # 1. Parse the document\n",
    "    path = f\"./sample_reports/{report_id}.pdf\"\n",
    "    pages, _ = parse_pdf(path=path)\n",
    "\n",
    "    # 2. Chunk the text\n",
    "    chunks, metadata = chunk_text(pages, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "\n",
    "    # 3. Generate and store vector representations\n",
    "    db_path = f\"./faiss_db_RetrievalSettings3/{report_id}\"\n",
    "    vectorstore = get_vectorstore(chunks, metadata, db_path, embedding_model=embeddings_qwen)\n",
    "\n",
    "    # 4. Retrieve the relevant chunks\n",
    "    result = retrieve_chunks(vectorstore, queries=QUERIES, report_id=report_id, top_k=TOP_K)\n",
    "    retrieved_chunks.update(result)\n",
    "    report_ids.append(report_id)\n",
    "\n",
    "# 5. Generate LLM answers\n",
    "    # 1. Prepare prompts\n",
    "prompts, metadata, final_analysis = prepare_prompts(\n",
    "    report_list=report_ids,\n",
    "    section_text_dict=retrieved_chunks\n",
    ")\n",
    "    # 2. Run inference\n",
    "model_responses = run_batched_inference(prompts, generate_text)\n",
    "    # 3. Parse and finalize results\n",
    "final_analysis = parse_results(\n",
    "    model_responses,\n",
    "    metadata,\n",
    "    final_analysis\n",
    ")\n",
    "\n",
    "time_per_report = (time.time() - start_time) / len(report_ids) / 60  # in minutes\n",
    "print(\"Processing time per report in minutes\", time_per_report)\n",
    "\n",
    "evaluate_verdicts(\n",
    "    validation_set,\n",
    "    final_analysis,\n",
    "    config_name=\"Retrieval Settings 3\",\n",
    "    llm_name=\"Llama 3.1 8B - Instruct\",\n",
    "    embedding_model=\"Qwen 0.6B\",\n",
    "    resources=f\"{time_per_report:.2f} min/report\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d86a6d32-8a17-4145-9391-9e5079afd174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Config                        LLM  \\\n",
      "0                   Baseline 1  OpenAI: gpt-4o-2024-08-06   \n",
      "1                   Baseline 2    Llama 3.1 8B - Instruct   \n",
      "2                   Baseline 3    Llama 3.1 8B - Instruct   \n",
      "3                   Baseline 4   Llama 3.3 70B - Instruct   \n",
      "4                   Baseline 5   Llama 3.3 70B - Instruct   \n",
      "5  PDF Extraction: MyPuPDF4LLM    Llama 3.1 8B - Instruct   \n",
      "6  PDF Extraction: MyPuPDF4LLM    Llama 3.1 8B - Instruct   \n",
      "7         Retrieval Settings 1    Llama 3.1 8B - Instruct   \n",
      "8         Retrieval Settings 2    Llama 3.1 8B - Instruct   \n",
      "9         Retrieval Settings 3    Llama 3.1 8B - Instruct   \n",
      "\n",
      "                  Embedding Model  Accuracy  Recall  Precision  F1-Score  \\\n",
      "0  OpenAI: text-embedding-ada-002     0.808   0.808      0.811     0.796   \n",
      "1                       Qwen 0.6B     0.769   0.769      0.788     0.774   \n",
      "2                         Qwen 4B     0.700   0.700      0.733     0.708   \n",
      "3                       Qwen 0.6B     0.738   0.738      0.731     0.733   \n",
      "4                         Qwen 4B     0.715   0.715      0.706     0.709   \n",
      "5                       Qwen 0.6B     0.654   0.654      0.744     0.661   \n",
      "6                       Qwen 0.6B     0.654   0.654      0.744     0.661   \n",
      "7                       Qwen 0.6B     0.754   0.754      0.764     0.757   \n",
      "8                       Qwen 0.6B     0.731   0.731      0.739     0.734   \n",
      "9                       Qwen 0.6B     0.738   0.738      0.729     0.729   \n",
      "\n",
      "          Resources  TOP_K  CHUNK_SIZE  CHUNK_OVERLAP  \n",
      "0   6.96 min/report    NaN         NaN            NaN  \n",
      "1   1.85 min/report    NaN         NaN            NaN  \n",
      "2   9.32 min/report    NaN         NaN            NaN  \n",
      "3  14.77 min/report    NaN         NaN            NaN  \n",
      "4  21.45 min/report    NaN         NaN            NaN  \n",
      "5   5.31 min/report    NaN         NaN            NaN  \n",
      "6   5.31 min/report   10.0       400.0           50.0  \n",
      "7   3.47 min/report   10.0       400.0           50.0  \n",
      "8   3.45 min/report   10.0       400.0          100.0  \n",
      "9   3.27 min/report   20.0       500.0           20.0  \n"
     ]
    }
   ],
   "source": [
    "# read in old summary\n",
    "old_comparison = pd.read_excel('rag_comparison.xlsx')\n",
    "\n",
    "df_new = pd.DataFrame(summary_results)  # Your newly calculated metrics\n",
    "summary_results = []\n",
    "df_new[\"TOP_K\"] = TOP_K\n",
    "df_new[\"CHUNK_SIZE\"] = CHUNK_SIZE\n",
    "df_new[\"CHUNK_OVERLAP\"] = CHUNK_OVERLAP\n",
    "df_summary = pd.concat([old_comparison, df_new], ignore_index=True)\n",
    "\n",
    "# Save back to the same file\n",
    "df_summary.to_excel(\"rag_comparison.xlsx\", index=False)\n",
    "print(df_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca0cbf2-f661-4c74-9fcf-9da641cbc5e0",
   "metadata": {},
   "source": [
    "## 4.\n",
    "- Chunk size 400\n",
    "- chunk overlap 100\n",
    "- top K 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c658610-671f-46a0-bd67-819052543277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Processing: SchneiderElectric_2024\n",
      "\n",
      " Processing: ContinentalAG_2024\n",
      "--- Step 1: Preparing all prompts ---\n",
      "\n",
      "--- Step 2: Sending 130 prompts to the pipeline ---\n",
      "Generated LLM answers — Computation time: 4.09 minutes\n",
      "\n",
      "--- Step 3: Parsing all responses ---\n",
      "Processing time per report in minutes 3.5524419963359835\n",
      "\n",
      "--- LLM Evaluation: Evaluating performance on verdicts ---\n",
      "\n",
      " Dropped 0 of 130 queries due to missing verdicts.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.754, 'precision': 0.754, 'recall': 0.754, 'f1_score': 0.754}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOP_K = 12\n",
    "CHUNK_SIZE = 400\n",
    "CHUNK_OVERLAP = 100\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Your pipeline execution \n",
    "retrieved_chunks = {}  # Holds retrieved text chunks per report\n",
    "report_ids = []        # Stores report IDs for final LLM analysis\n",
    "\n",
    "# Loop through each report in your sample DataFrame\n",
    "for idx, row in sample.iterrows():\n",
    "    company_name = row['company_withAccessInfo']\n",
    "    report_id = f\"{prepare_filename(company_name)}_2024\".replace(\" \", \"\")\n",
    "    print(f\"\\n Processing: {report_id}\")\n",
    "\n",
    "    # 1. Parse the document\n",
    "    path = f\"./sample_reports/{report_id}.pdf\"\n",
    "    pages, _ = parse_pdf(path=path)\n",
    "\n",
    "    # 2. Chunk the text\n",
    "    chunks, metadata = chunk_text(pages, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "\n",
    "    # 3. Generate and store vector representations\n",
    "    db_path = f\"./faiss_db_RetrievalSettings4/{report_id}\"\n",
    "    vectorstore = get_vectorstore(chunks, metadata, db_path, embedding_model=embeddings_qwen)\n",
    "\n",
    "    # 4. Retrieve the relevant chunks\n",
    "    result = retrieve_chunks(vectorstore, queries=QUERIES, report_id=report_id, top_k=TOP_K)\n",
    "    retrieved_chunks.update(result)\n",
    "    report_ids.append(report_id)\n",
    "\n",
    "# 5. Generate LLM answers\n",
    "    # 1. Prepare prompts\n",
    "prompts, metadata, final_analysis = prepare_prompts(\n",
    "    report_list=report_ids,\n",
    "    section_text_dict=retrieved_chunks\n",
    ")\n",
    "    # 2. Run inference\n",
    "model_responses = run_batched_inference(prompts, generate_text)\n",
    "    # 3. Parse and finalize results\n",
    "final_analysis = parse_results(\n",
    "    model_responses,\n",
    "    metadata,\n",
    "    final_analysis\n",
    ")\n",
    "\n",
    "time_per_report = (time.time() - start_time) / len(report_ids) / 60  # in minutes\n",
    "print(\"Processing time per report in minutes\", time_per_report)\n",
    "\n",
    "evaluate_verdicts(\n",
    "    validation_set,\n",
    "    final_analysis,\n",
    "    config_name=\"Retrieval Settings 4\",\n",
    "    llm_name=\"Llama 3.1 8B - Instruct\",\n",
    "    embedding_model=\"Qwen 0.6B\",\n",
    "    resources=f\"{time_per_report:.2f} min/report\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57a6f073-005b-4ae6-bbd9-c034de96c9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Config                        LLM  \\\n",
      "0                    Baseline 1  OpenAI: gpt-4o-2024-08-06   \n",
      "1                    Baseline 2    Llama 3.1 8B - Instruct   \n",
      "2                    Baseline 3    Llama 3.1 8B - Instruct   \n",
      "3                    Baseline 4   Llama 3.3 70B - Instruct   \n",
      "4                    Baseline 5   Llama 3.3 70B - Instruct   \n",
      "5   PDF Extraction: MyPuPDF4LLM    Llama 3.1 8B - Instruct   \n",
      "6   PDF Extraction: MyPuPDF4LLM    Llama 3.1 8B - Instruct   \n",
      "7          Retrieval Settings 1    Llama 3.1 8B - Instruct   \n",
      "8          Retrieval Settings 2    Llama 3.1 8B - Instruct   \n",
      "9          Retrieval Settings 3    Llama 3.1 8B - Instruct   \n",
      "10         Retrieval Settings 4    Llama 3.1 8B - Instruct   \n",
      "\n",
      "                   Embedding Model  Accuracy  Recall  Precision  F1-Score  \\\n",
      "0   OpenAI: text-embedding-ada-002     0.808   0.808      0.811     0.796   \n",
      "1                        Qwen 0.6B     0.769   0.769      0.788     0.774   \n",
      "2                          Qwen 4B     0.700   0.700      0.733     0.708   \n",
      "3                        Qwen 0.6B     0.738   0.738      0.731     0.733   \n",
      "4                          Qwen 4B     0.715   0.715      0.706     0.709   \n",
      "5                        Qwen 0.6B     0.654   0.654      0.744     0.661   \n",
      "6                        Qwen 0.6B     0.654   0.654      0.744     0.661   \n",
      "7                        Qwen 0.6B     0.754   0.754      0.764     0.757   \n",
      "8                        Qwen 0.6B     0.731   0.731      0.739     0.734   \n",
      "9                        Qwen 0.6B     0.738   0.738      0.729     0.729   \n",
      "10                       Qwen 0.6B     0.754   0.754      0.754     0.754   \n",
      "\n",
      "           Resources  TOP_K  CHUNK_SIZE  CHUNK_OVERLAP  \n",
      "0    6.96 min/report    NaN         NaN            NaN  \n",
      "1    1.85 min/report    NaN         NaN            NaN  \n",
      "2    9.32 min/report    NaN         NaN            NaN  \n",
      "3   14.77 min/report    NaN         NaN            NaN  \n",
      "4   21.45 min/report    NaN         NaN            NaN  \n",
      "5    5.31 min/report    NaN         NaN            NaN  \n",
      "6    5.31 min/report   10.0       400.0           50.0  \n",
      "7    3.47 min/report   10.0       400.0           50.0  \n",
      "8    3.45 min/report   10.0       400.0          100.0  \n",
      "9    3.27 min/report   20.0       500.0           20.0  \n",
      "10   3.55 min/report   12.0       400.0          100.0  \n"
     ]
    }
   ],
   "source": [
    "# read in old summary\n",
    "old_comparison = pd.read_excel('rag_comparison.xlsx')\n",
    "\n",
    "df_new = pd.DataFrame(summary_results)  # Your newly calculated metrics\n",
    "summary_results = []\n",
    "df_new[\"TOP_K\"] = TOP_K\n",
    "df_new[\"CHUNK_SIZE\"] = CHUNK_SIZE\n",
    "df_new[\"CHUNK_OVERLAP\"] = CHUNK_OVERLAP\n",
    "df_summary = pd.concat([old_comparison, df_new], ignore_index=True)\n",
    "\n",
    "# Save back to the same file\n",
    "df_summary.to_excel(\"rag_comparison.xlsx\", index=False)\n",
    "print(df_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea22f46b-7c71-4a91-ab94-c1e82c90954a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Processing: SchneiderElectric_2024\n",
      "\n",
      " Processing: ContinentalAG_2024\n",
      "--- Step 1: Preparing all prompts ---\n",
      "\n",
      "--- Step 2: Sending 130 prompts to the pipeline ---\n",
      "Generated LLM answers — Computation time: 3.61 minutes\n",
      "\n",
      "--- Step 3: Parsing all responses ---\n",
      "Processing time per report in minutes 3.1158853590488436\n",
      "\n",
      "--- LLM Evaluation: Evaluating performance on verdicts ---\n",
      "\n",
      " Dropped 0 of 130 queries due to missing verdicts.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.777, 'precision': 0.793, 'recall': 0.777, 'f1_score': 0.781}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOP_K = 8\n",
    "CHUNK_SIZE = 350\n",
    "CHUNK_OVERLAP = 50\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Your pipeline execution \n",
    "retrieved_chunks = {}  # Holds retrieved text chunks per report\n",
    "report_ids = []        # Stores report IDs for final LLM analysis\n",
    "\n",
    "# Loop through each report in your sample DataFrame\n",
    "for idx, row in sample.iterrows():\n",
    "    company_name = row['company_withAccessInfo']\n",
    "    report_id = f\"{prepare_filename(company_name)}_2024\".replace(\" \", \"\")\n",
    "    print(f\"\\n Processing: {report_id}\")\n",
    "\n",
    "    # 1. Parse the document\n",
    "    path = f\"./sample_reports/{report_id}.pdf\"\n",
    "    pages, _ = parse_pdf(path=path)\n",
    "\n",
    "    # 2. Chunk the text\n",
    "    chunks, metadata = chunk_text(pages, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "\n",
    "    # 3. Generate and store vector representations\n",
    "    db_path = f\"./faiss_db_again/{report_id}\"\n",
    "    vectorstore = get_vectorstore(chunks, metadata, db_path, embedding_model=embeddings_qwen)\n",
    "\n",
    "    # 4. Retrieve the relevant chunks\n",
    "    result = retrieve_chunks(vectorstore, queries=QUERIES, report_id=report_id, top_k=TOP_K)\n",
    "    retrieved_chunks.update(result)\n",
    "    report_ids.append(report_id)\n",
    "\n",
    "# 5. Generate LLM answers\n",
    "    # 1. Prepare prompts\n",
    "prompts, metadata, final_analysis = prepare_prompts(\n",
    "    report_list=report_ids,\n",
    "    section_text_dict=retrieved_chunks\n",
    ")\n",
    "    # 2. Run inference\n",
    "model_responses = run_batched_inference(prompts, generate_text)\n",
    "    # 3. Parse and finalize results\n",
    "final_analysis = parse_results(\n",
    "    model_responses,\n",
    "    metadata,\n",
    "    final_analysis\n",
    ")\n",
    "\n",
    "time_per_report = (time.time() - start_time) / len(report_ids) / 60  # in minutes\n",
    "print(\"Processing time per report in minutes\", time_per_report)\n",
    "\n",
    "evaluate_verdicts(\n",
    "    validation_set,\n",
    "    final_analysis,\n",
    "    config_name=\"Retrieval Settings 4\",\n",
    "    llm_name=\"Llama 3.1 8B - Instruct\",\n",
    "    embedding_model=\"Qwen 0.6B\",\n",
    "    resources=f\"{time_per_report:.2f} min/report\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myKernel_thesis",
   "language": "python",
   "name": "mykernel_thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
