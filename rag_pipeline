import os
import re
import io
import gc
import json
import time
import requests
import torch
import pandas as pd
import transformers
import pymupdf
from dotenv import load_dotenv
from huggingface_hub import login
from torch import cuda
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.prompts import PromptTemplate


# =============================
# Environment Setup
# =============================
os.environ["TOKENIZERS_PARALLELISM"] = "false"
dotenv_path = os.path.expanduser("~/thesis/esg_extraction/.env")
load_dotenv(dotenv_path=dotenv_path)
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
HF_TOKEN = os.getenv("HUGGINGFACE_TOKEN")
login(HF_TOKEN)


# =============================
# Helper Functions
# =============================
def prepare_filename(name):
    return re.sub(r'[\\/*?:"<>|]', "", name)


def _docs_to_string(docs, with_source=True):
    output = ""
    for doc in docs:
        output += "Content: {}\n".format(doc.page_content)
        if with_source:
            output += "Source: {}\n".format(doc.metadata['page'])
        output += "\n---\n"
    return output


def _find_answer(full_text):
    try:
        for line in full_text.splitlines():
            if "ANSWER" in line:
                idx = line.find(":") + 1
                return line[idx:].strip().strip('",')
        return full_text.strip()
    except Exception:
        return full_text.strip()


def _find_verdict(answer_text):
    if not answer_text:
        return "N/A"
    match = re.search(r'\[\[\s*(YES|NO)\s*\]\]', answer_text, re.IGNORECASE)
    if match:
        return match.group(1).upper()
    return "N/A"


def _find_sources(full_text):
    sources_match = re.search(r"SOURCES\s*:\s*\[([^\]]+)\]", full_text)
    if sources_match:
        number_list = re.findall(r'\d+', sources_match.group(1))
        return [int(n) for n in number_list]
    return [int(n) for n in re.findall(r'\b\d{3,5}\b', full_text)]


# =============================
# Read in ESRS Metadata
# =============================
esrs_metadata = pd.read_excel('../EsrsMetadata.xlsx')
QUERIES = dict(zip(esrs_metadata["query_id"], esrs_metadata["query"]))
GUIDELINES = dict(zip(esrs_metadata["query_id"], esrs_metadata["guidelines"]))


# =============================
# Document Parsing
# =============================
def parse_pdf(path=None, url=None):
    assert (path is not None) != (url is not None), "Provide either a local path or a URL."
    if path:
        pdf = pymupdf.open(path)
    else:
        response = requests.get(url)
        pdf = pymupdf.open(stream=io.BytesIO(response.content), filetype='pdf')
    pages = [page.get_text() for page in pdf]
    full_text = ''.join(pages)
    return pages, full_text


# =============================
# Text Chunking
# =============================
def chunk_text(pages, chunk_size, chunk_overlap):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len,
        separators=["\n\n", "\n", " "]
    )
    chunks = []
    metadata = []
    for idx, page in enumerate(pages):
        page_chunks = splitter.split_text(page)
        chunks.extend(page_chunks)
        metadata.extend([{"page": str(idx + 1)}] * len(page_chunks))
    return chunks, metadata


# =============================
# Vectorstore
# =============================
def get_vectorstore(chunks, metadata, db_path, embedding_model):
    if os.path.exists(db_path):
        vectorstore = FAISS.load_local(db_path, embeddings=embedding_model, allow_dangerous_deserialization=True)
    else:
        vectorstore = FAISS.from_texts(chunks, embedding_model, metadatas=metadata)
        vectorstore.save_local(db_path)
    return vectorstore


# =============================
# Retrieval
# =============================
def retrieve_chunks(vectorstore, queries, report_id, top_k):
    retriever = vectorstore.as_retriever(search_kwargs={"k": top_k})
    section_text_dict = {}
    for key, query in queries.items():
        section_text_dict[key] = retriever.invoke(query)
    return {report_id: section_text_dict}


# =============================
# Prompt Template
# =============================
TOP_K = 8
CHUNK_SIZE = 350
CHUNK_OVERLAP = 50
ANSWER_LENGTH = 200
MAX_TOKEN = 500

PROMPT_TEMPLATE = ("""
You are a senior sustainabiliy analyst with expertise in the european reporting standards evaluating a company's disclosure on social sustainability.


You are presented with the following sources from the company's annual report:
--------------------- [BEGIN OF SOURCES]\n
{sources}\n
--------------------- [END OF SOURCES]\n

Given the sources information and no prior knowledge, your main task is to respond to the posed question encapsulated in "||".
Question: ||{query}||

Please consider the following additional explanation to the question encapsulated in "+++++" as crucial for answering the question:
+++++ [BEGIN OF EXPLANATION]
{guideline}
+++++ [END OF EXPLANATION]

### Response Instructions ###
Please enforce to the following guidelines in your ANSWER:
1. Your response must be precise, thorough, and grounded on specific extracts from the report to verify its authenticity.
2. If you are unsure, simply acknowledge the lack of knowledge, rather than fabricating an answer.
3. Be skeptical to the information disclosed in the report as there might be greenwashing (exaggerating the firm's environmental responsibility). Always answer in a critical tone.
4. Cheap talks are statements that are costless to make and may not necessarily reflect the true intentions or future actions of the company. Be critical for all cheap talks you discovered in the report.
5. Always acknowledge that the information provided is representing the company's view based on its report.
6. Scrutinize whether the report is grounded in quantifiable, concrete data or vague, unverifiable statements, and communicate your findings.
7. Start your ANSWER with a "[[YES]]"" or ""[[NO]]"" depending on whether you would answer the question with a yes or no. Always complement your judgement on yes or no with a short explanation that summarizes the sources in an informative way, i.e. provide details.
8. Keep your ANSWER within {answer_length} words.

### Formatting Instructions ###
- Format your answer in JSON format with the two keys: ANSWER (this should contain your answer string without sources), and SOURCES (this should be a list of the SOURCE numbers that were referenced in your answer).
- Your response **must** be returned as a **valid JSON object**.
- Only output the JSON object â€” no preamble, no markdown, no extra commentary.
- Use this exact format for your final output:
{{
  "ANSWER": "[[YES]] or [[NO]] Here follows your explanation",
  "SOURCES": ["1", "216", "181-182", "174"]
}}

Your FINAL_ANSWER in JSON (ensure there's no format error):
""")

disclosure_prompt = PromptTemplate(
    template=PROMPT_TEMPLATE,
    input_variables=["query", "sources", "guideline", "answer_length"]
)

SYSTEM_PROMPT = "You are an AI assistant in the role of a Senior Equity Analyst with expertise in sustainability reporting that analyzes companys' annual reports."

# =============================
# Model Setup
# =============================
BATCH_SIZE = 64
TEMPERATURE = 0.01

llm_name = "meta-llama/Llama-3.1-8B-Instruct"
device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'

tokenizer = AutoTokenizer.from_pretrained(llm_name)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.pad_token_id = tokenizer.eos_token_id
tokenizer.padding_side = "left"

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

model = AutoModelForCausalLM.from_pretrained(
    llm_name,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    quantization_config=quantization_config
)

model.eval()

generate_text = transformers.pipeline(
    model=model,
    tokenizer=tokenizer,
    task='text-generation',
    return_full_text=True,
    temperature=TEMPERATURE,
    max_new_tokens=MAX_TOKEN,
    batch_size=BATCH_SIZE
)

embeddings_qwen = HuggingFaceEmbeddings(
    model_name="Qwen/Qwen3-Embedding-0.6B",
    model_kwargs={'device': 'cuda'}
)

# =============================
# Inference Functions
# =============================
def prepare_prompts(report_list, section_text_dict):
    prompts_to_process = []
    metadata_for_prompts = []
    final_results = {report: {} for report in report_list}

    for report in report_list:
        for key, query_text in QUERIES.items():
            context_str = _docs_to_string(section_text_dict[report].get(key, []))
            if not context_str.strip():
                final_results[report][key] = {
                    "verdict": "NO",
                    "analysis": "No relevant context was found to answer the question.",
                    "sources": []
                }
                continue
            prompt_text = disclosure_prompt.format(
                query=query_text,
                sources=context_str,
                guideline=GUIDELINES.get(key, ""),
                answer_length=ANSWER_LENGTH
            )
            prompts_to_process.append([
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": prompt_text}
            ])
            metadata_for_prompts.append({"report": report, "key": key})
    return prompts_to_process, metadata_for_prompts, final_results


def run_batched_inference(prompts, generate_text_pipeline, batch_size=BATCH_SIZE):
    if not prompts:
        return []
    start_time = time.time()
    responses = generate_text_pipeline(prompts, batch_size=batch_size)
    torch.cuda.empty_cache()
    return responses


def parse_results(responses, metadata, existing_results):
    for meta, response in zip(metadata, responses):
        report = meta["report"]
        key = meta["key"]
        full_text = response[0]['generated_text'][-1]['content']
        json_match = re.search(r'\{.*?\}', full_text, re.DOTALL)

        if json_match:
            json_str = json_match.group(0)
            try:
                parsed_json = json.loads(json_str)
                answer = parsed_json.get("ANSWER", "")
                sources = parsed_json.get("SOURCES", [])
            except json.JSONDecodeError:
                answer = _find_answer(full_text)
                sources = _find_sources(full_text)
        else:
            answer = _find_answer(full_text)
            sources = _find_sources(full_text)

        verdict = _find_verdict(answer)
        if verdict in [None, "N/A"]:
            answer = "N/A"
            sources = "N/A"

        existing_results[report][key] = {
            "verdict": verdict,
            "analysis": answer,
            "sources": sources
        }

        gc.collect()
        torch.cuda.empty_cache()
    return existing_results

